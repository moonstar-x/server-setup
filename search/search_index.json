{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 This is a simple guide that focuses on the installation and configuration process of my little home server that runs multiple services for various purposes composed of two computers. The server now uses Docker for most of its services. This is an improved version of the previous server guide , some of the services from the previous version are present here, some are gone for good.","title":"Home"},{"location":"#introduction","text":"This is a simple guide that focuses on the installation and configuration process of my little home server that runs multiple services for various purposes composed of two computers. The server now uses Docker for most of its services. This is an improved version of the previous server guide , some of the services from the previous version are present here, some are gone for good.","title":"Introduction"},{"location":"bootcamp/games/","text":"Introduction \u00b6 This section describes the game servers installed on the Windows partition of the Mac Mini server. Game server installation is based off my own game-server-updater script. You can check it out if you're interested on using it. Further sections for game servers downloaded with SteamCMD will include the commands necessary to install them since the script already includes them. This guide assumes you're using said script to install the servers. If you're interested, I have uploaded all the configs for these servers in a separate repo which are referenced all over the following sections.","title":"Introduction"},{"location":"bootcamp/games/#introduction","text":"This section describes the game servers installed on the Windows partition of the Mac Mini server. Game server installation is based off my own game-server-updater script. You can check it out if you're interested on using it. Further sections for game servers downloaded with SteamCMD will include the commands necessary to install them since the script already includes them. This guide assumes you're using said script to install the servers. If you're interested, I have uploaded all the configs for these servers in a separate repo which are referenced all over the following sections.","title":"Introduction"},{"location":"bootcamp/games/arma3/","text":"Arma 3 \u00b6 Use this script to install the game server. This script is a bit different from the rest because it has a post run function that will automatically create symlinks for the mod folders and will update the mod keys inside the keys folder of the server. Running this script is enough to get the server ready with mods installed. Configuration \u00b6 Inside the server folder you should create a scripts folder and then use subfolders for each server configuration you need. In my case, one of my subfolders is named horror_cup . Inside this subfolder, you should have a server.cfg file. You can base yourself off the following: Running the Server \u00b6 To run this server you should create a script inside the subfolder of the scripts folder. In my case, one of my subfolders is named horror_cup . Inside this subfolder, you should have a start.bat file. You can base yourself off the following: The MODLIST variable contains the names of the symlinked folders for the mods to load on the server. This should be a string with the names of the mod folders separated by a ; character. For unix users, the ; character should be escaped ( \\; ). Running the Server with Headless Client \u00b6 If you wish to run the server with a headless client alongside the server on the same machine, you can base yourself of the following start.bat file.","title":"Arma 3"},{"location":"bootcamp/games/arma3/#arma-3","text":"Use this script to install the game server. This script is a bit different from the rest because it has a post run function that will automatically create symlinks for the mod folders and will update the mod keys inside the keys folder of the server. Running this script is enough to get the server ready with mods installed.","title":"Arma 3"},{"location":"bootcamp/games/arma3/#configuration","text":"Inside the server folder you should create a scripts folder and then use subfolders for each server configuration you need. In my case, one of my subfolders is named horror_cup . Inside this subfolder, you should have a server.cfg file. You can base yourself off the following:","title":"Configuration"},{"location":"bootcamp/games/arma3/#running-the-server","text":"To run this server you should create a script inside the subfolder of the scripts folder. In my case, one of my subfolders is named horror_cup . Inside this subfolder, you should have a start.bat file. You can base yourself off the following: The MODLIST variable contains the names of the symlinked folders for the mods to load on the server. This should be a string with the names of the mod folders separated by a ; character. For unix users, the ; character should be escaped ( \\; ).","title":"Running the Server"},{"location":"bootcamp/games/arma3/#running-the-server-with-headless-client","text":"If you wish to run the server with a headless client alongside the server on the same machine, you can base yourself of the following start.bat file.","title":"Running the Server with Headless Client"},{"location":"bootcamp/games/csgo/","text":"Counter Strike: Global Offensive \u00b6 Use this script to install the game server. Sourcemod \u00b6 In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the csgo folder of the server. Additionally, you need to download a metamod.vdf . Inside the link, choose the appropriate game and download the file. This file should be placed inside csgo/addons . Updating Admins \u00b6 Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the csgo/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID. Plugins \u00b6 Currently this server does not have any plugins set up. Configuration \u00b6 Inside the csgo/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config: Running the Server \u00b6 To run this server you should run the following command: srcds.exe -game csgo -console -usercon -tickrate 128 +game_type 1 +game_mode 2 +mapgroup mg_active +map de_mirage With a Script \u00b6 You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"Counter-Strike: Global Offensive"},{"location":"bootcamp/games/csgo/#counter-strike-global-offensive","text":"Use this script to install the game server.","title":"Counter Strike: Global Offensive"},{"location":"bootcamp/games/csgo/#sourcemod","text":"In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the csgo folder of the server. Additionally, you need to download a metamod.vdf . Inside the link, choose the appropriate game and download the file. This file should be placed inside csgo/addons .","title":"Sourcemod"},{"location":"bootcamp/games/csgo/#updating-admins","text":"Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the csgo/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID.","title":"Updating Admins"},{"location":"bootcamp/games/csgo/#plugins","text":"Currently this server does not have any plugins set up.","title":"Plugins"},{"location":"bootcamp/games/csgo/#configuration","text":"Inside the csgo/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config:","title":"Configuration"},{"location":"bootcamp/games/csgo/#running-the-server","text":"To run this server you should run the following command: srcds.exe -game csgo -console -usercon -tickrate 128 +game_type 1 +game_mode 2 +mapgroup mg_active +map de_mirage","title":"Running the Server"},{"location":"bootcamp/games/csgo/#with-a-script","text":"You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"With a Script"},{"location":"bootcamp/games/dayz/","text":"DayZ \u00b6 Use this script to install the game server. Configuration \u00b6 Inside the server folder you should create a scripts folder and then use subfolders for each server configuration you need. In my case, one of my subfolders is named regular . Inside this subfolder, you should have a server.cfg file. You can base yourself off the following: Running the Server \u00b6 To run this server you should create a script inside the subfolder of the scripts folder. In my case, one of my subfolders is named regular . Inside this subfolder, you should have a start.bat file. You can base yourself off the following:","title":"DayZ"},{"location":"bootcamp/games/dayz/#dayz","text":"Use this script to install the game server.","title":"DayZ"},{"location":"bootcamp/games/dayz/#configuration","text":"Inside the server folder you should create a scripts folder and then use subfolders for each server configuration you need. In my case, one of my subfolders is named regular . Inside this subfolder, you should have a server.cfg file. You can base yourself off the following:","title":"Configuration"},{"location":"bootcamp/games/dayz/#running-the-server","text":"To run this server you should create a script inside the subfolder of the scripts folder. In my case, one of my subfolders is named regular . Inside this subfolder, you should have a start.bat file. You can base yourself off the following:","title":"Running the Server"},{"location":"bootcamp/games/l4d2/","text":"Left 4 Dead 2 \u00b6 Use this script to install the game server. Sourcemod \u00b6 In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the left4dead2 folder of the server. Updating Admins \u00b6 Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the left4dead2/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID. Plugins \u00b6 Currently this server has the following plugins: AutoBhop \u00b6 This plugin allows for easier bunny hopping. You can download it from here . The .smx file should be placed inside left4dead2/addons/sourcemod/plugins . Configuration \u00b6 Inside the left4dead2/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config: Running the Server \u00b6 To run this server you should run the following command: srcds.exe -game left4dead2 -console -maxplayers 4 +map c1m1_hotel With a Script \u00b6 You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"Left 4 Dead 2"},{"location":"bootcamp/games/l4d2/#left-4-dead-2","text":"Use this script to install the game server.","title":"Left 4 Dead 2"},{"location":"bootcamp/games/l4d2/#sourcemod","text":"In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the left4dead2 folder of the server.","title":"Sourcemod"},{"location":"bootcamp/games/l4d2/#updating-admins","text":"Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the left4dead2/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID.","title":"Updating Admins"},{"location":"bootcamp/games/l4d2/#plugins","text":"Currently this server has the following plugins:","title":"Plugins"},{"location":"bootcamp/games/l4d2/#autobhop","text":"This plugin allows for easier bunny hopping. You can download it from here . The .smx file should be placed inside left4dead2/addons/sourcemod/plugins .","title":"AutoBhop"},{"location":"bootcamp/games/l4d2/#configuration","text":"Inside the left4dead2/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config:","title":"Configuration"},{"location":"bootcamp/games/l4d2/#running-the-server","text":"To run this server you should run the following command: srcds.exe -game left4dead2 -console -maxplayers 4 +map c1m1_hotel","title":"Running the Server"},{"location":"bootcamp/games/l4d2/#with-a-script","text":"You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"With a Script"},{"location":"bootcamp/games/nmrih/","text":"No More Room in Hell \u00b6 Use this script to install the game server. Sourcemod \u00b6 In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the nrmih folder of the server. Updating Admins \u00b6 Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the nrmih/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID. Plugins \u00b6 Currently this server does not have any plugins set up. Configuration \u00b6 Inside the nrmih/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config: Running the Server \u00b6 To run this server you should run the following command: srcds.exe -game nmrih -console -maxplayers 8 +map nmo_broadway With a Script \u00b6 You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"No More Room in Hell"},{"location":"bootcamp/games/nmrih/#no-more-room-in-hell","text":"Use this script to install the game server.","title":"No More Room in Hell"},{"location":"bootcamp/games/nmrih/#sourcemod","text":"In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the nrmih folder of the server.","title":"Sourcemod"},{"location":"bootcamp/games/nmrih/#updating-admins","text":"Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the nrmih/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID.","title":"Updating Admins"},{"location":"bootcamp/games/nmrih/#plugins","text":"Currently this server does not have any plugins set up.","title":"Plugins"},{"location":"bootcamp/games/nmrih/#configuration","text":"Inside the nrmih/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config:","title":"Configuration"},{"location":"bootcamp/games/nmrih/#running-the-server","text":"To run this server you should run the following command: srcds.exe -game nmrih -console -maxplayers 8 +map nmo_broadway","title":"Running the Server"},{"location":"bootcamp/games/nmrih/#with-a-script","text":"You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"With a Script"},{"location":"bootcamp/games/reactivedrop/","text":"Alien Swarm Reactive Drop \u00b6 Use this script to install the game server. Sourcemod \u00b6 In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the reactivedrop folder of the server. Updating Admins \u00b6 Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the reactivedrop/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID. Plugins \u00b6 Currently this server does not have any plugins set up. Configuration \u00b6 Inside the reactivedrop/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config: Additionally to this file, you need a workshop.cfg file inside reactivedrop/cfg and include commands to download workshop content into the server (not necessary though). Running the Server \u00b6 To run this server you should run the following command: srcds.exe -game reactivedrop -console -maxplayers 8 +map lobby With a Script \u00b6 You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"Alien Swarm Reactive Drop"},{"location":"bootcamp/games/reactivedrop/#alien-swarm-reactive-drop","text":"Use this script to install the game server.","title":"Alien Swarm Reactive Drop"},{"location":"bootcamp/games/reactivedrop/#sourcemod","text":"In order to add plugins on your server, you'll need Sourcemod and Metamod , click on the mentioned links to download the necessary files. These archives will include folders named addons and cfg which should be placed inside the reactivedrop folder of the server.","title":"Sourcemod"},{"location":"bootcamp/games/reactivedrop/#updating-admins","text":"Some commands will require you to be an admin. First, you should get your steam user's STEAM32 ID which can be found here . Once you have this, append the following line inside the reactivedrop/addons/sourcemod/configs/admins_simple.ini file: STEAM_0:1:23456789 99:z Info Replace STEAM_0:1:23456789 with your actual Steam ID.","title":"Updating Admins"},{"location":"bootcamp/games/reactivedrop/#plugins","text":"Currently this server does not have any plugins set up.","title":"Plugins"},{"location":"bootcamp/games/reactivedrop/#configuration","text":"Inside the reactivedrop/cfg folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config: Additionally to this file, you need a workshop.cfg file inside reactivedrop/cfg and include commands to download workshop content into the server (not necessary though).","title":"Configuration"},{"location":"bootcamp/games/reactivedrop/#running-the-server","text":"To run this server you should run the following command: srcds.exe -game reactivedrop -console -maxplayers 8 +map lobby","title":"Running the Server"},{"location":"bootcamp/games/reactivedrop/#with-a-script","text":"You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"With a Script"},{"location":"bootcamp/games/svencoop/","text":"Sven Coop \u00b6 Use this script to install the game server. Configuration \u00b6 Inside the svencoop folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config: Running the Server \u00b6 To run this server you should run the following command: svends.exe -console +maxplayers 8 +log on +map _server_start With a Script \u00b6 You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"Sven Coop"},{"location":"bootcamp/games/svencoop/#sven-coop","text":"Use this script to install the game server.","title":"Sven Coop"},{"location":"bootcamp/games/svencoop/#configuration","text":"Inside the svencoop folder you should create a server.cfg and add the commands to configure your server. You may base yourself from my own config:","title":"Configuration"},{"location":"bootcamp/games/svencoop/#running-the-server","text":"To run this server you should run the following command: svends.exe -console +maxplayers 8 +log on +map _server_start","title":"Running the Server"},{"location":"bootcamp/games/svencoop/#with-a-script","text":"You may also create a bat script to run this server easily. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"With a Script"},{"location":"bootcamp/games/valheim/","text":"Valheim \u00b6 Use this script to install the game server. Running the Server \u00b6 To run this server you need to create a script file. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"Valheim"},{"location":"bootcamp/games/valheim/#valheim","text":"Use this script to install the game server.","title":"Valheim"},{"location":"bootcamp/games/valheim/#running-the-server","text":"To run this server you need to create a script file. I like to create a scripts folder inside the root of the server folder and add whatever scripts I need in there. For this case, the scripts/start.bat file contains the following:","title":"Running the Server"},{"location":"linux/services/cloudflared/","text":"Cloudflared (Native) \u00b6 Cloudflared is a daemon that allows to manage Cloudflare Argo Tunnel . We'll use this to expose our web services through Cloudflare without the need to port forward and expose our home network directly. Installation \u00b6 To install Cloudflared : wget -q https://bin.equinox.io/c/VdrWdbjqyF/cloudflared-stable-linux-amd64.deb sudo dpkg -i cloudflared-stable-linux-amd64.deb Authenticating \u00b6 We now need to authenticate with Cloudflare, for this run: cloudflared tunnel login A link will be displayed on the console. Visit it on any browser, login and select the zone you wish to enable Argo Tunnel for. This will create a cert.pem file inside ~/.cloudflared . Creating a Tunnel \u00b6 We can create as many tunnels as we want. Generally one tunnel per DNS zone should suffice. To create a tunnel, run: cloudflared tunnel create <NAME> Note Replace <NAME> with the desired name for your tunnel. Once done, this will create a .json file inside ~/.cloudflared that contains the tunnel credentials. Pre-Configuration \u00b6 Since we'll create two different tunnels for two different domains, we'll create a folder in ~/.cloudflared with the name of the tunnel lower-cased, where we'll move cert.pem and UUID.json files. Configuration \u00b6 Inside ~/.cloudflared/<name> we'll create a config.yml file with the configuration of our tunnel. tunnel : TUNNEL_ID credentials-file : /home/USER/.cloudflared/NAME/TUNNEL_ID.json ingress : - hostname : subdomain.domain.com service : http://localhost:port - hostname : subdomain.domain.com service : http://localhost:port - service : http_status:404 Note You can add as many hostname/service pairs as you want. Running \u00b6 You can run the tunnel with: cloudflared tunnel --config ~/.cloudflared/name/config.yml --origincert ~/.cloudflared/name/cert.pem run Auto-starting \u00b6 To auto-start the tunnel, create a service file: sudo nano /etc/systemd/system/cloudflared_name.service And add the following: [Unit] Description=Cloudflared Tunnel for $NAME After=network.target [Service] Type=simple User=$USER WorkingDirectory=/home/$USER/.cloudflared/$NAME ExecStart=/usr/local/bin/cloudflared tunnel --config ./config.yml --origincert ./cert.pem run Restart=always [Install] WantedBy=multi-user.target Note Replace $USER and $NAME with the correct values for your tunnel. Once saved, start and enable the service. sudo systemctl start cloudflared_name.service sudo systemctl enable cloudflared_name.service Last Words \u00b6 Repeat the process from Authenticating for each tunnel created.","title":"Cloudflared (Native)"},{"location":"linux/services/cloudflared/#cloudflared-native","text":"Cloudflared is a daemon that allows to manage Cloudflare Argo Tunnel . We'll use this to expose our web services through Cloudflare without the need to port forward and expose our home network directly.","title":"Cloudflared (Native)"},{"location":"linux/services/cloudflared/#installation","text":"To install Cloudflared : wget -q https://bin.equinox.io/c/VdrWdbjqyF/cloudflared-stable-linux-amd64.deb sudo dpkg -i cloudflared-stable-linux-amd64.deb","title":"Installation"},{"location":"linux/services/cloudflared/#authenticating","text":"We now need to authenticate with Cloudflare, for this run: cloudflared tunnel login A link will be displayed on the console. Visit it on any browser, login and select the zone you wish to enable Argo Tunnel for. This will create a cert.pem file inside ~/.cloudflared .","title":"Authenticating"},{"location":"linux/services/cloudflared/#creating-a-tunnel","text":"We can create as many tunnels as we want. Generally one tunnel per DNS zone should suffice. To create a tunnel, run: cloudflared tunnel create <NAME> Note Replace <NAME> with the desired name for your tunnel. Once done, this will create a .json file inside ~/.cloudflared that contains the tunnel credentials.","title":"Creating a Tunnel"},{"location":"linux/services/cloudflared/#pre-configuration","text":"Since we'll create two different tunnels for two different domains, we'll create a folder in ~/.cloudflared with the name of the tunnel lower-cased, where we'll move cert.pem and UUID.json files.","title":"Pre-Configuration"},{"location":"linux/services/cloudflared/#configuration","text":"Inside ~/.cloudflared/<name> we'll create a config.yml file with the configuration of our tunnel. tunnel : TUNNEL_ID credentials-file : /home/USER/.cloudflared/NAME/TUNNEL_ID.json ingress : - hostname : subdomain.domain.com service : http://localhost:port - hostname : subdomain.domain.com service : http://localhost:port - service : http_status:404 Note You can add as many hostname/service pairs as you want.","title":"Configuration"},{"location":"linux/services/cloudflared/#running","text":"You can run the tunnel with: cloudflared tunnel --config ~/.cloudflared/name/config.yml --origincert ~/.cloudflared/name/cert.pem run","title":"Running"},{"location":"linux/services/cloudflared/#auto-starting","text":"To auto-start the tunnel, create a service file: sudo nano /etc/systemd/system/cloudflared_name.service And add the following: [Unit] Description=Cloudflared Tunnel for $NAME After=network.target [Service] Type=simple User=$USER WorkingDirectory=/home/$USER/.cloudflared/$NAME ExecStart=/usr/local/bin/cloudflared tunnel --config ./config.yml --origincert ./cert.pem run Restart=always [Install] WantedBy=multi-user.target Note Replace $USER and $NAME with the correct values for your tunnel. Once saved, start and enable the service. sudo systemctl start cloudflared_name.service sudo systemctl enable cloudflared_name.service","title":"Auto-starting"},{"location":"linux/services/cloudflared/#last-words","text":"Repeat the process from Authenticating for each tunnel created.","title":"Last Words"},{"location":"linux/services/samba/","text":"Samba (Native) \u00b6 Samba lets your Linux based server share files and folders on a Windows File Sharing Workgroup using the same protocol (SMB/CIFS), this is pretty useful when you need to share files between computers on your network. This also helps to allow your files to be accessed through the Internet (although it should only be done through a VPN for security purposes). Installation \u00b6 To install Samba : sudo apt-get install samba Configuring \u00b6 We'll make a folder on our 2TB SATA hard drive directory, this will serve as the share folder which will be accessible through SMB . mkdir /media/sata_2tb/sambashare We now need to add this folder entry to the configuration. Open up Samba 's config file. sudo nano /etc/samba/smb.conf Now we'll add the folder we created alongside other directories to the Samba share (note that you should replace these directories corresponding to your needs). You can add as many entries as you need as long as they're well formatted. [sambashare] comment = Samba shared folder path = /media/sata_2tb/sambashare read only = no browsable = yes writeable = yes Now we restart the Samba service to apply the changes: sudo service smbd restart Before trying to access the share with another computer, we'll need to add a password to the Samba user. Use the next command and replace $USER with your username. sudo smbpasswd -a $USER Now, as a final step, add the firewall rule to allow SMB connections. sudo ufw allow 445 /tcp Issues with Permissions on Windows \u00b6 In case you try to access a shared folder that doesn't have the corresponding permissions to allow the Windows user to manage said folder, you can add the following directives to each shared folder block in the config file: create umask = 0777 directory umask = 0777 Shared Folders that Contain Symlinks \u00b6 If one of your shared folders has symlinks in them and you need to share them too, add the following directives to the shared folder block in the config file: follow symlinks = yes wide links = yes Browsing a Shared Folder as a Specified User \u00b6 If you want your shared folder to be accessed as a certain user, add the following directive to the shared folder block in the config file: force user = $USER Note Replace $USER with the UNIX username required.","title":"Samba (Native)"},{"location":"linux/services/samba/#samba-native","text":"Samba lets your Linux based server share files and folders on a Windows File Sharing Workgroup using the same protocol (SMB/CIFS), this is pretty useful when you need to share files between computers on your network. This also helps to allow your files to be accessed through the Internet (although it should only be done through a VPN for security purposes).","title":"Samba (Native)"},{"location":"linux/services/samba/#installation","text":"To install Samba : sudo apt-get install samba","title":"Installation"},{"location":"linux/services/samba/#configuring","text":"We'll make a folder on our 2TB SATA hard drive directory, this will serve as the share folder which will be accessible through SMB . mkdir /media/sata_2tb/sambashare We now need to add this folder entry to the configuration. Open up Samba 's config file. sudo nano /etc/samba/smb.conf Now we'll add the folder we created alongside other directories to the Samba share (note that you should replace these directories corresponding to your needs). You can add as many entries as you need as long as they're well formatted. [sambashare] comment = Samba shared folder path = /media/sata_2tb/sambashare read only = no browsable = yes writeable = yes Now we restart the Samba service to apply the changes: sudo service smbd restart Before trying to access the share with another computer, we'll need to add a password to the Samba user. Use the next command and replace $USER with your username. sudo smbpasswd -a $USER Now, as a final step, add the firewall rule to allow SMB connections. sudo ufw allow 445 /tcp","title":"Configuring"},{"location":"linux/services/samba/#issues-with-permissions-on-windows","text":"In case you try to access a shared folder that doesn't have the corresponding permissions to allow the Windows user to manage said folder, you can add the following directives to each shared folder block in the config file: create umask = 0777 directory umask = 0777","title":"Issues with Permissions on Windows"},{"location":"linux/services/samba/#shared-folders-that-contain-symlinks","text":"If one of your shared folders has symlinks in them and you need to share them too, add the following directives to the shared folder block in the config file: follow symlinks = yes wide links = yes","title":"Shared Folders that Contain Symlinks"},{"location":"linux/services/samba/#browsing-a-shared-folder-as-a-specified-user","text":"If you want your shared folder to be accessed as a certain user, add the following directive to the shared folder block in the config file: force user = $USER Note Replace $USER with the UNIX username required.","title":"Browsing a Shared Folder as a Specified User"},{"location":"linux/services/analytics/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to analytics servers. mkdir ~/analytics For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/analytics/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to analytics servers. mkdir ~/analytics For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/analytics/plausible/","text":"Plausible \u00b6 Plausible is a Web analytics dashboard. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the analytics server's data will be saved. mkdir ~/analytics/plausible Next, create a folder for the Clickhouse configs inside this new folder: mkdir clickhouse And create the following files: nano clickhouse/clickhouse-config.xml <clickhouse> <logger> <level> warning </level> <console> true </console> </logger> <!-- Stop all the unnecessary logging --> <query_thread_log remove= \"remove\" /> <query_log remove= \"remove\" /> <text_log remove= \"remove\" /> <trace_log remove= \"remove\" /> <metric_log remove= \"remove\" /> <asynchronous_metric_log remove= \"remove\" /> <session_log remove= \"remove\" /> <part_log remove= \"remove\" /> </clickhouse> nano clickhouse/clickhouse-user-config.xml <clickhouse> <profiles> <default> <log_queries> 0 </log_queries> <log_query_threads> 0 </log_query_threads> </default> </profiles> </clickhouse> And now change the permissions for this folder: chmod -R 777 clickhouse Docker Compose \u00b6 Plausible will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : plausible : image : plausible/analytics:v1 restart : unless-stopped command : sh -c \"sleep 10 && /entrypoint.sh db createdb && /entrypoint.sh db migrate && /entrypoint.sh run\" depends_on : - db - events_db ports : - 10500:80 environment : - TZ=America/Guayaquil - BASE_URL=CHANGE_THIS_URL - PORT=80 - SECRET_KEY_BASE=CHANGE_THIS_SECRET_KEY - DISABLE_REGISTRATION=true - DATABASE_URL=postgres://plausible:CHANGE_THIS_DB_PASS@db/plausible?sslmode=disable - CLICKHOUSE_DATABASE_URL=http://events_db:8123/plausible_events_db db : image : postgres:14-alpine restart : unless-stopped volumes : - ./db-data:/var/lib/postgresql/data environment : - POSTGRES_USER=plausible - POSTGRES_PASSWORD=CHANGE_THIS_DB_PASS events_db : image : clickhouse/clickhouse-server:22-alpine restart : unless-stopped volumes : - ./events-data:/var/lib/clickhouse - ./clickhouse/clickhouse-config.xml:/etc/clickhouse-server/config.d/logging.xml:ro - ./clickhouse/clickhouse-user-config.xml:/etc/clickhouse-server/users.d/logging.xml:ro ulimits : nofile : soft : 262144 hard : 262144 environment : - CLICKHOUSE_DB=plausible_events_db Note Make sure to change CHANGE_THIS_URL with the URL of where this service will be hosted with protocol. For instance, https://analytics.example.com . Make sure to change CHANGE_THIS_DB_PASS with anything you want. Make sure to change CHANGE_THIS_SECRET_KEY with a random key. You may use the openssl rand -hex 64 command to generate one. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 10500 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Plausible"},{"location":"linux/services/analytics/plausible/#plausible","text":"Plausible is a Web analytics dashboard. This service has an official image available on Docker Hub which we'll use.","title":"Plausible"},{"location":"linux/services/analytics/plausible/#pre-installation","text":"We'll create a folder in the main user's home where all the analytics server's data will be saved. mkdir ~/analytics/plausible Next, create a folder for the Clickhouse configs inside this new folder: mkdir clickhouse And create the following files: nano clickhouse/clickhouse-config.xml <clickhouse> <logger> <level> warning </level> <console> true </console> </logger> <!-- Stop all the unnecessary logging --> <query_thread_log remove= \"remove\" /> <query_log remove= \"remove\" /> <text_log remove= \"remove\" /> <trace_log remove= \"remove\" /> <metric_log remove= \"remove\" /> <asynchronous_metric_log remove= \"remove\" /> <session_log remove= \"remove\" /> <part_log remove= \"remove\" /> </clickhouse> nano clickhouse/clickhouse-user-config.xml <clickhouse> <profiles> <default> <log_queries> 0 </log_queries> <log_query_threads> 0 </log_query_threads> </default> </profiles> </clickhouse> And now change the permissions for this folder: chmod -R 777 clickhouse","title":"Pre-Installation"},{"location":"linux/services/analytics/plausible/#docker-compose","text":"Plausible will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : plausible : image : plausible/analytics:v1 restart : unless-stopped command : sh -c \"sleep 10 && /entrypoint.sh db createdb && /entrypoint.sh db migrate && /entrypoint.sh run\" depends_on : - db - events_db ports : - 10500:80 environment : - TZ=America/Guayaquil - BASE_URL=CHANGE_THIS_URL - PORT=80 - SECRET_KEY_BASE=CHANGE_THIS_SECRET_KEY - DISABLE_REGISTRATION=true - DATABASE_URL=postgres://plausible:CHANGE_THIS_DB_PASS@db/plausible?sslmode=disable - CLICKHOUSE_DATABASE_URL=http://events_db:8123/plausible_events_db db : image : postgres:14-alpine restart : unless-stopped volumes : - ./db-data:/var/lib/postgresql/data environment : - POSTGRES_USER=plausible - POSTGRES_PASSWORD=CHANGE_THIS_DB_PASS events_db : image : clickhouse/clickhouse-server:22-alpine restart : unless-stopped volumes : - ./events-data:/var/lib/clickhouse - ./clickhouse/clickhouse-config.xml:/etc/clickhouse-server/config.d/logging.xml:ro - ./clickhouse/clickhouse-user-config.xml:/etc/clickhouse-server/users.d/logging.xml:ro ulimits : nofile : soft : 262144 hard : 262144 environment : - CLICKHOUSE_DB=plausible_events_db Note Make sure to change CHANGE_THIS_URL with the URL of where this service will be hosted with protocol. For instance, https://analytics.example.com . Make sure to change CHANGE_THIS_DB_PASS with anything you want. Make sure to change CHANGE_THIS_SECRET_KEY with a random key. You may use the openssl rand -hex 64 command to generate one.","title":"Docker Compose"},{"location":"linux/services/analytics/plausible/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 10500 /tcp","title":"Post-Installation"},{"location":"linux/services/analytics/plausible/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/automation/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to automation related services. mkdir ~/automation For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/automation/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to automation related services. mkdir ~/automation For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/automation/automation-service/","text":"Custom Automation Service \u00b6 This service is completely custom made for my needs so chances are this will not be of any use for you. The only reason I went out of my way to create my own service was because I was finding myself using way too much the Code node in n8n . Given this, maintaining my workflows wasn't really that easy and at this point it would be a lot better for me to just maintain an actual code based workflow system rather than the workflows I had there. This service has an Docker image hosted on GitHub Packages which will be used. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/automation/automation-service Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : service : image : ghcr.io/moonstar-x/automation-service:latest restart : unless-stopped user : 1000:1000 volumes : - ./data:/opt/app/data - ./config:/opt/app/config ports : - 5678:5678 environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 5678 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Custom Automation Service"},{"location":"linux/services/automation/automation-service/#custom-automation-service","text":"This service is completely custom made for my needs so chances are this will not be of any use for you. The only reason I went out of my way to create my own service was because I was finding myself using way too much the Code node in n8n . Given this, maintaining my workflows wasn't really that easy and at this point it would be a lot better for me to just maintain an actual code based workflow system rather than the workflows I had there. This service has an Docker image hosted on GitHub Packages which will be used.","title":"Custom Automation Service"},{"location":"linux/services/automation/automation-service/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/automation/automation-service","title":"Pre-Installation"},{"location":"linux/services/automation/automation-service/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : service : image : ghcr.io/moonstar-x/automation-service:latest restart : unless-stopped user : 1000:1000 volumes : - ./data:/opt/app/data - ./config:/opt/app/config ports : - 5678:5678 environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/automation/automation-service/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 5678 /tcp","title":"Post-Installation"},{"location":"linux/services/automation/automation-service/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/automation/smart-home/","text":"Smart Home \u00b6 Note This will probably become a multi-service stack in the future. Homebridge is an IoT bridge for HomeKit that brings support to iOS's HomeKit to smart home devices that originally do not provide support for it. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the automation server's data will be saved. mkdir ~/automation/smart-home Docker Compose \u00b6 Smart Home will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : service : image : oznu/homebridge:latest restart : unless-stopped network_mode : host volumes : - ./homebridge:/homebridge environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8581 /tcp sudo ufw allow 51845 /tcp sudo ufw allow 51845 /udp Note Make sure to check the 51845 port applies for you, I believe this port is chosen at random when setting-up Homebridge . You can check this by reading the output logs when setting-up this service. Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Smart Home"},{"location":"linux/services/automation/smart-home/#smart-home","text":"Note This will probably become a multi-service stack in the future. Homebridge is an IoT bridge for HomeKit that brings support to iOS's HomeKit to smart home devices that originally do not provide support for it. This service has an official image available on Docker Hub which we'll use.","title":"Smart Home"},{"location":"linux/services/automation/smart-home/#pre-installation","text":"We'll create a folder in the main user's home where all the automation server's data will be saved. mkdir ~/automation/smart-home","title":"Pre-Installation"},{"location":"linux/services/automation/smart-home/#docker-compose","text":"Smart Home will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : service : image : oznu/homebridge:latest restart : unless-stopped network_mode : host volumes : - ./homebridge:/homebridge environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/automation/smart-home/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8581 /tcp sudo ufw allow 51845 /tcp sudo ufw allow 51845 /udp Note Make sure to check the 51845 port applies for you, I believe this port is chosen at random when setting-up Homebridge . You can check this by reading the output logs when setting-up this service.","title":"Post-Installation"},{"location":"linux/services/automation/smart-home/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/data/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to data servers. mkdir ~/data For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/data/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to data servers. mkdir ~/data For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/data/actual/","text":"Actual \u00b6 Actual is a self-hosted budgeting application. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/actual Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : actual : image : actualbudget/actual-server:latest restart : unless-stopped volumes : - ./data:/data ports : - 5006:5006 environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 5006 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Actual"},{"location":"linux/services/data/actual/#actual","text":"Actual is a self-hosted budgeting application. This service has an official image on Docker Hub which we'll use.","title":"Actual"},{"location":"linux/services/data/actual/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/actual","title":"Pre-Installation"},{"location":"linux/services/data/actual/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : actual : image : actualbudget/actual-server:latest restart : unless-stopped volumes : - ./data:/data ports : - 5006:5006 environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/data/actual/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 5006 /tcp","title":"Post-Installation"},{"location":"linux/services/data/actual/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/data/gitea/","text":"Gitea \u00b6 Gitea is a self-hosted git server, useful for having a private VCS solution. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/gitea Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : gitea : image : gitea/gitea:latest restart : unless-stopped volumes : - ./data:/data ports : - 3000:3000 depends_on : - db environment : - TZ=America/Guayaquil db : image : mariadb:10 restart : unless-stopped volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=gitea - MYSQL_USER=gitea - MYSQL_PASSWORD=CHANGE_THIS Note Make sure to change CHANGE_THIS to a custom value. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 3000 /tcp Once you have started the server once, edit the config file located inside the data volume: nano data/gitea/conf/app.ini And make sure to have the following lines: [service] DISABLE_REGISTRATION = true This will make sure that nobody else can register into your server without your knowledge. Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Gitea"},{"location":"linux/services/data/gitea/#gitea","text":"Gitea is a self-hosted git server, useful for having a private VCS solution. This service has an official image on Docker Hub which we'll use.","title":"Gitea"},{"location":"linux/services/data/gitea/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/gitea","title":"Pre-Installation"},{"location":"linux/services/data/gitea/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : gitea : image : gitea/gitea:latest restart : unless-stopped volumes : - ./data:/data ports : - 3000:3000 depends_on : - db environment : - TZ=America/Guayaquil db : image : mariadb:10 restart : unless-stopped volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=gitea - MYSQL_USER=gitea - MYSQL_PASSWORD=CHANGE_THIS Note Make sure to change CHANGE_THIS to a custom value.","title":"Docker Compose"},{"location":"linux/services/data/gitea/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 3000 /tcp Once you have started the server once, edit the config file located inside the data volume: nano data/gitea/conf/app.ini And make sure to have the following lines: [service] DISABLE_REGISTRATION = true This will make sure that nobody else can register into your server without your knowledge.","title":"Post-Installation"},{"location":"linux/services/data/gitea/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/data/jenkins/","text":"Jenkins \u00b6 Jenkins is a CI/CD service. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/jenkins Agent Dockerfile \u00b6 We'll use a custom Dockerfile to create an agent that has the necessary dependencies installed. First, create a new folder for the agent's data. mkdir agent Then, we'll generate an SSH key that Jenkins will use to connect to the agent. ssh-keygen agent/jenkins_agent_key Now, run the following command: id And check for the ID of the docker group. In this case, this value is 998 . Next, we'll add a Dockerfile for the agent: FROM jenkins/ssh-agent:jdk11 USER root RUN groupadd -g 998 docker RUN apt-get update -qq RUN apt-get install -qqy apt-transport-https ca-certificates curl gnupg2 software-properties-common RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg RUN echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $( lsb_release -cs ) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null RUN apt-get update -qq && apt-get -y install docker-ce RUN usermod -aG docker jenkins Make sure to add the generated private key as a Credential inside Jenkins as an SSH Username with private key with jenkins as the username. Docker Compose \u00b6 Jenkins will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : jenkins : image : jenkins/jenkins:lts restart : unless-stopped volumes : - ./data:/var/jenkins_home ports : - 10800:8080 environment : - TZ=America/Guayaquil agent : build : ./agent/ restart : unless-stopped depends_on : - jenkins volumes : - /var/run/docker.sock:/var/run/docker.sock environment : - TZ=America/Guayaquil - JENKINS_AGENT_SSH_PUBKEY=ssh-rsa SECRET_REPLACE_THIS Post-Installation \u00b6 To avoid permission issues, create the data folder that will be used as a volume: mkdir data We'll need to allow the service's port on our firewall. sudo ufw allow 10800 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Jenkins"},{"location":"linux/services/data/jenkins/#jenkins","text":"Jenkins is a CI/CD service. This service has an official image available on Docker Hub which we'll use.","title":"Jenkins"},{"location":"linux/services/data/jenkins/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/jenkins","title":"Pre-Installation"},{"location":"linux/services/data/jenkins/#agent-dockerfile","text":"We'll use a custom Dockerfile to create an agent that has the necessary dependencies installed. First, create a new folder for the agent's data. mkdir agent Then, we'll generate an SSH key that Jenkins will use to connect to the agent. ssh-keygen agent/jenkins_agent_key Now, run the following command: id And check for the ID of the docker group. In this case, this value is 998 . Next, we'll add a Dockerfile for the agent: FROM jenkins/ssh-agent:jdk11 USER root RUN groupadd -g 998 docker RUN apt-get update -qq RUN apt-get install -qqy apt-transport-https ca-certificates curl gnupg2 software-properties-common RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg RUN echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $( lsb_release -cs ) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null RUN apt-get update -qq && apt-get -y install docker-ce RUN usermod -aG docker jenkins Make sure to add the generated private key as a Credential inside Jenkins as an SSH Username with private key with jenkins as the username.","title":"Agent Dockerfile"},{"location":"linux/services/data/jenkins/#docker-compose","text":"Jenkins will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : jenkins : image : jenkins/jenkins:lts restart : unless-stopped volumes : - ./data:/var/jenkins_home ports : - 10800:8080 environment : - TZ=America/Guayaquil agent : build : ./agent/ restart : unless-stopped depends_on : - jenkins volumes : - /var/run/docker.sock:/var/run/docker.sock environment : - TZ=America/Guayaquil - JENKINS_AGENT_SSH_PUBKEY=ssh-rsa SECRET_REPLACE_THIS","title":"Docker Compose"},{"location":"linux/services/data/jenkins/#post-installation","text":"To avoid permission issues, create the data folder that will be used as a volume: mkdir data We'll need to allow the service's port on our firewall. sudo ufw allow 10800 /tcp","title":"Post-Installation"},{"location":"linux/services/data/jenkins/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/data/nextcloud/","text":"Nextcloud \u00b6 Nextcloud is a self-hosted cloud data server, useful for keeping documents in your own server. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/nextcloud Dockerfile \u00b6 Despite the service having an official image available, it lacks a dependency necessary to allow for SMB shares as external storage. The following is the (tiny) Dockerfile that will be used for this service: FROM nextcloud:stable RUN apt-get update && apt-get install -y procps smbclient && rm -rf /var/lib/apt/lists/* Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : nextcloud : build : . restart : unless-stopped volumes : - /media/sata_2tb/Nextcloud:/var/www/html ports : - 9020:80 depends_on : - db environment : - TZ=America/Guayaquil - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=CHANGE_THIS - MYSQL_HOST=db db : image : mariadb:10.5 restart : unless-stopped command : --transaction-isolation=READ-COMMITTED --binlog-format=ROW volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=CHANGE_THIS cron : image : nextcloud:stable restart : unless-stopped depends_on : - nextcloud volumes : - /media/sata_2tb/Nextcloud:/var/www/html entrypoint : /cron.sh environment : - TZ=America/Guayaquil Note Make sure to change CHANGE_THIS to a custom value. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 9020 /tcp We'll also need to add the user to the www-data group to allow access to the data being uploaded inside the service: sudo usermod -aG www-data $USER Additionally, you may need to change the /media/sata_2tb/Nextcloud/config/config.php file. Make sure to have the correct array of trusted_domains , to update the overwrite.cli.url to the correct URL where the service will be accessible from, and include the following line: 'overwriteprotocol' => 'https', Inside Nextcloud, some applications should be installed, notably the Group Shared Folder one, which allows to quickly share folders with multiple users internally. Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Nextcloud"},{"location":"linux/services/data/nextcloud/#nextcloud","text":"Nextcloud is a self-hosted cloud data server, useful for keeping documents in your own server. This service has an official image on Docker Hub which we'll use.","title":"Nextcloud"},{"location":"linux/services/data/nextcloud/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/data/nextcloud","title":"Pre-Installation"},{"location":"linux/services/data/nextcloud/#dockerfile","text":"Despite the service having an official image available, it lacks a dependency necessary to allow for SMB shares as external storage. The following is the (tiny) Dockerfile that will be used for this service: FROM nextcloud:stable RUN apt-get update && apt-get install -y procps smbclient && rm -rf /var/lib/apt/lists/*","title":"Dockerfile"},{"location":"linux/services/data/nextcloud/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : nextcloud : build : . restart : unless-stopped volumes : - /media/sata_2tb/Nextcloud:/var/www/html ports : - 9020:80 depends_on : - db environment : - TZ=America/Guayaquil - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=CHANGE_THIS - MYSQL_HOST=db db : image : mariadb:10.5 restart : unless-stopped command : --transaction-isolation=READ-COMMITTED --binlog-format=ROW volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=CHANGE_THIS cron : image : nextcloud:stable restart : unless-stopped depends_on : - nextcloud volumes : - /media/sata_2tb/Nextcloud:/var/www/html entrypoint : /cron.sh environment : - TZ=America/Guayaquil Note Make sure to change CHANGE_THIS to a custom value.","title":"Docker Compose"},{"location":"linux/services/data/nextcloud/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 9020 /tcp We'll also need to add the user to the www-data group to allow access to the data being uploaded inside the service: sudo usermod -aG www-data $USER Additionally, you may need to change the /media/sata_2tb/Nextcloud/config/config.php file. Make sure to have the correct array of trusted_domains , to update the overwrite.cli.url to the correct URL where the service will be accessible from, and include the following line: 'overwriteprotocol' => 'https', Inside Nextcloud, some applications should be installed, notably the Group Shared Folder one, which allows to quickly share folders with multiple users internally.","title":"Post-Installation"},{"location":"linux/services/data/nextcloud/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/discord-bots/","text":"Initialization \u00b6 All the bots inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the bots, we'll create a folder on the main user's home folder dedicated to Discord bots. mkdir ~/discord For each bot created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/discord-bots/#initialization","text":"All the bots inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the bots, we'll create a folder on the main user's home folder dedicated to Discord bots. mkdir ~/discord For each bot created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/discord-bots/free-games-notifier/","text":"Free Games Notifier \u00b6 discord-free-games-notifier is a bot that notifies a channel whenever there's a free game on Steam or Epic Games. This bot has an image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/discord-free-games-notifier Docker Compose \u00b6 The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : bot : image : moonstarx/discord-free-games-notifier:latest restart : unless-stopped network_mode : host volumes : - ./data:/opt/app/data environment : - DISCORD_TOKEN=<DISCORD_TOKEN_HERE> - DISCORD_PREFIX=! - DISCORD_OWNER_ID=<OWNER_ID_HERE> - TZ=America/Guayaquil Running \u00b6 Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Free Games Notifier"},{"location":"linux/services/discord-bots/free-games-notifier/#free-games-notifier","text":"discord-free-games-notifier is a bot that notifies a channel whenever there's a free game on Steam or Epic Games. This bot has an image available on Docker Hub which we'll use.","title":"Free Games Notifier"},{"location":"linux/services/discord-bots/free-games-notifier/#pre-installation","text":"We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/discord-free-games-notifier","title":"Pre-Installation"},{"location":"linux/services/discord-bots/free-games-notifier/#docker-compose","text":"The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : bot : image : moonstarx/discord-free-games-notifier:latest restart : unless-stopped network_mode : host volumes : - ./data:/opt/app/data environment : - DISCORD_TOKEN=<DISCORD_TOKEN_HERE> - DISCORD_PREFIX=! - DISCORD_OWNER_ID=<OWNER_ID_HERE> - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/discord-bots/free-games-notifier/#running","text":"Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/discord-bots/jda-music-bot/","text":"JDA Music Bot \u00b6 jagrosh's MusicBot is a very powerful music bot written in Java with JDA. This bot does not have an official Docker image so we'll create a Dockerfile for this one. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/jda-music-bot We'll also need to download the latest version of the jar from here which will be downloaded inside an app folder where all the bot's data will be stored. cd ~/discord/jda-music-bot && mkdir app && cd app wget https://github.com/jagrosh/MusicBot/releases/download/0.3.4/JMusicBot-0.3.4.jar mv JMusicBot-0.3.4.jar JMusicBot.jar Now, we should have a JMusicBot.jar file inside of ~/discord/jda-music-bot/app . Configuration \u00b6 This bot requires its configuration to be saved in a text file. We'll create a file called config.txt with: nano ~/discord/jda-music-bot/app/config.txt And its content should be as follows: token=<DISCORD_TOKEN_HERE> owner=<OWNER_ID_HERE> prefix=% game=\"DEFAULT\" status=ONLINE songinstatus=true altprefix=\"NONE\" stayinchannel=true maxtime=0 updatealerts=false Dockerfile \u00b6 Since the bot does not have an official Docker image, we'll create a Dockerfile to run any java jar file through volumes. The content of the Dockerfile file is as follows: FROM openjdk:8 WORKDIR /opt/app VOLUME /opt/app CMD [ \"java\" , \"-Dnogui=true\" , \"-jar\" , \"JMusicBot.jar\" ] Docker Compose \u00b6 The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : bot : build : . restart : unless-stopped network_mode : host volumes : - ./app:/opt/app environment : - TZ=America/Guayaquil Before starting, we need to build this image, do so with: docker-compose build Running \u00b6 Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"JDA Music Bot"},{"location":"linux/services/discord-bots/jda-music-bot/#jda-music-bot","text":"jagrosh's MusicBot is a very powerful music bot written in Java with JDA. This bot does not have an official Docker image so we'll create a Dockerfile for this one.","title":"JDA Music Bot"},{"location":"linux/services/discord-bots/jda-music-bot/#pre-installation","text":"We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/jda-music-bot We'll also need to download the latest version of the jar from here which will be downloaded inside an app folder where all the bot's data will be stored. cd ~/discord/jda-music-bot && mkdir app && cd app wget https://github.com/jagrosh/MusicBot/releases/download/0.3.4/JMusicBot-0.3.4.jar mv JMusicBot-0.3.4.jar JMusicBot.jar Now, we should have a JMusicBot.jar file inside of ~/discord/jda-music-bot/app .","title":"Pre-Installation"},{"location":"linux/services/discord-bots/jda-music-bot/#configuration","text":"This bot requires its configuration to be saved in a text file. We'll create a file called config.txt with: nano ~/discord/jda-music-bot/app/config.txt And its content should be as follows: token=<DISCORD_TOKEN_HERE> owner=<OWNER_ID_HERE> prefix=% game=\"DEFAULT\" status=ONLINE songinstatus=true altprefix=\"NONE\" stayinchannel=true maxtime=0 updatealerts=false","title":"Configuration"},{"location":"linux/services/discord-bots/jda-music-bot/#dockerfile","text":"Since the bot does not have an official Docker image, we'll create a Dockerfile to run any java jar file through volumes. The content of the Dockerfile file is as follows: FROM openjdk:8 WORKDIR /opt/app VOLUME /opt/app CMD [ \"java\" , \"-Dnogui=true\" , \"-jar\" , \"JMusicBot.jar\" ]","title":"Dockerfile"},{"location":"linux/services/discord-bots/jda-music-bot/#docker-compose","text":"The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : bot : build : . restart : unless-stopped network_mode : host volumes : - ./app:/opt/app environment : - TZ=America/Guayaquil Before starting, we need to build this image, do so with: docker-compose build","title":"Docker Compose"},{"location":"linux/services/discord-bots/jda-music-bot/#running","text":"Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/discord-bots/music-24-7/","text":"Music 24/7 Bot \u00b6 discord-music-24-7 is a music bot with auto-pause capabilities that pauses the music playback when nobody is in the channel. It can play music from YouTube, SoundCloud and local files. This bot has an image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/discord-music-24-7 Docker Compose \u00b6 The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : bot : image : moonstarx/discord-music-24-7:latest restart : unless-stopped network_mode : host volumes : - ./data:/opt/app/data environment : - DISCORD_TOKEN=<DISCORD_TOKEN_HERE> - DISCORD_PREFIX=s! - DISCORD_CHANNEL_ID=<CHANNEL_ID_HERE> - DISCORD_OWNER_ID=<OWNER_ID_HERE> - TZ=America/Guayaquil Running \u00b6 Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Music 24/7 Bot"},{"location":"linux/services/discord-bots/music-24-7/#music-247-bot","text":"discord-music-24-7 is a music bot with auto-pause capabilities that pauses the music playback when nobody is in the channel. It can play music from YouTube, SoundCloud and local files. This bot has an image available on Docker Hub which we'll use.","title":"Music 24/7 Bot"},{"location":"linux/services/discord-bots/music-24-7/#pre-installation","text":"We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/discord-music-24-7","title":"Pre-Installation"},{"location":"linux/services/discord-bots/music-24-7/#docker-compose","text":"The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : bot : image : moonstarx/discord-music-24-7:latest restart : unless-stopped network_mode : host volumes : - ./data:/opt/app/data environment : - DISCORD_TOKEN=<DISCORD_TOKEN_HERE> - DISCORD_PREFIX=s! - DISCORD_CHANNEL_ID=<CHANNEL_ID_HERE> - DISCORD_OWNER_ID=<OWNER_ID_HERE> - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/discord-bots/music-24-7/#running","text":"Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/discord-bots/tts-bot/","text":"Text-to-Speech Bot \u00b6 discord-tts-bot is a bot that uses the Google Translate API to utter the messages you send to the bot in any language. This bot has an image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/discord-tts-bot Docker Compose \u00b6 The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : redis : image : redis:latest restart : unless-stopped ports : - 60000:6379 volumes : - ./data:/data environment : - TZ=America/Guayaquil command : redis-server --save 60 1 --loglevel warning bot : image : moonstarx/discord-tts-bot:latest restart : unless-stopped depends_on : - redis environment : - DISCORD_TOKEN=<DISCORD_TOKEN_HERE> - DISCORD_OWNER_ID=<OWNER_ID_HERE> - DISCORD_DEFAULT_DISCONNECT_TIMEOUT=10 - DISCORD_PROVIDER_TYPE=redis - DISCORD_REDIS_URL=redis://redis:6379 - TZ=America/Guayaquil Running \u00b6 Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Text-to-Speech Bot"},{"location":"linux/services/discord-bots/tts-bot/#text-to-speech-bot","text":"discord-tts-bot is a bot that uses the Google Translate API to utter the messages you send to the bot in any language. This bot has an image available on Docker Hub which we'll use.","title":"Text-to-Speech Bot"},{"location":"linux/services/discord-bots/tts-bot/#pre-installation","text":"We'll create a folder in the main user's home where all the bot's data will be saved. mkdir ~/discord/discord-tts-bot","title":"Pre-Installation"},{"location":"linux/services/discord-bots/tts-bot/#docker-compose","text":"The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : redis : image : redis:latest restart : unless-stopped ports : - 60000:6379 volumes : - ./data:/data environment : - TZ=America/Guayaquil command : redis-server --save 60 1 --loglevel warning bot : image : moonstarx/discord-tts-bot:latest restart : unless-stopped depends_on : - redis environment : - DISCORD_TOKEN=<DISCORD_TOKEN_HERE> - DISCORD_OWNER_ID=<OWNER_ID_HERE> - DISCORD_DEFAULT_DISCONNECT_TIMEOUT=10 - DISCORD_PROVIDER_TYPE=redis - DISCORD_REDIS_URL=redis://redis:6379 - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/discord-bots/tts-bot/#running","text":"Start up the bot with: docker-compose up -d That's it! The bot will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/docker/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to docker related services. mkdir ~/docker For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/docker/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to docker related services. mkdir ~/docker For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/docker/fleet/","text":"Fleet \u00b6 Fleet is a web UI that displays maintained Docker images from our own repositories, it serves a central place to display all your docker images. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/docker/fleet Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : fleet : image : ghcr.io/linuxserver/fleet:latest restart : unless-stopped volumes : - ./config:/config ports : - 8080:8080 depends_on : - db environment : - PUID=1000 - PGID=1000 - fleet_admin_authentication_type=DATABASE - fleet_database_url=jdbc:mariadb://db/fleet - fleet_database_username=fleet - fleet_database_password=CHANGE_THIS - TZ=America/Guayaquil db : image : mariadb:10 restart : unless-stopped volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=fleet - MYSQL_USER=fleet - MYSQL_PASSWORD=CHANGE_THIS Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note Make sure to change CHANGE_THIS to a custom value. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8080 /tcp The default user and password are: admin : admin , you should create a new user and remove this initial admin user. Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Fleet"},{"location":"linux/services/docker/fleet/#fleet","text":"Fleet is a web UI that displays maintained Docker images from our own repositories, it serves a central place to display all your docker images. This service has an official image on Docker Hub which we'll use.","title":"Fleet"},{"location":"linux/services/docker/fleet/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/docker/fleet","title":"Pre-Installation"},{"location":"linux/services/docker/fleet/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : fleet : image : ghcr.io/linuxserver/fleet:latest restart : unless-stopped volumes : - ./config:/config ports : - 8080:8080 depends_on : - db environment : - PUID=1000 - PGID=1000 - fleet_admin_authentication_type=DATABASE - fleet_database_url=jdbc:mariadb://db/fleet - fleet_database_username=fleet - fleet_database_password=CHANGE_THIS - TZ=America/Guayaquil db : image : mariadb:10 restart : unless-stopped volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=fleet - MYSQL_USER=fleet - MYSQL_PASSWORD=CHANGE_THIS Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note Make sure to change CHANGE_THIS to a custom value.","title":"Docker Compose"},{"location":"linux/services/docker/fleet/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8080 /tcp The default user and password are: admin : admin , you should create a new user and remove this initial admin user.","title":"Post-Installation"},{"location":"linux/services/docker/fleet/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/docker/portainer/","text":"Portainer \u00b6 Portainer is a web UI for Docker which allows us to have an insight on all the containers running on our server. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/docker/portainer Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : portainer : image : portainer/portainer-ce:latest restart : unless-stopped volumes : - ./data:/data - /var/run/docker.sock:/var/run/docker.sock ports : - 8000:8000 - 9000:9000 environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8000 /tcp sudo ufw allow 9000 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Portainer"},{"location":"linux/services/docker/portainer/#portainer","text":"Portainer is a web UI for Docker which allows us to have an insight on all the containers running on our server. This service has an official image on Docker Hub which we'll use.","title":"Portainer"},{"location":"linux/services/docker/portainer/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/docker/portainer","title":"Pre-Installation"},{"location":"linux/services/docker/portainer/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : portainer : image : portainer/portainer-ce:latest restart : unless-stopped volumes : - ./data:/data - /var/run/docker.sock:/var/run/docker.sock ports : - 8000:8000 - 9000:9000 environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/docker/portainer/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8000 /tcp sudo ufw allow 9000 /tcp","title":"Post-Installation"},{"location":"linux/services/docker/portainer/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/games/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to game servers and related services. mkdir ~/games For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/games/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to game servers and related services. mkdir ~/games For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/games/assettocorsa/","text":"Assetto Corsa \u00b6 Assetto Corsa is a realistic racing simulator. This game server has a community made server manager available on Docker Hub , however, I have made a small fork of this to update the source for SteamCMD since I've been having quite a lot of trouble getting it to work. This fork is available from GitHub Packages , which is the image we'll use for this. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the game server's data will be saved. mkdir ~/games/assettocorsa Docker Compose \u00b6 The game server will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : assettocorsa : image : ghcr.io/moonstar-x/assetto-server-manager:master restart : unless-stopped user : 1000:1000 ports : - 8772:8772 - 9600:9600 - 9600:9600/udp - 8081:8081 volumes : - ./server:/home/assetto/server-manager/assetto - ./data:/home/assetto/server-manager Warning Make sure to create the data and server folder before starting the container, otherwise you'll have some problems with the server data being saved. Configuration \u00b6 Create a config file inside data/config.yml : nano data/config.yml And paste the following: steam : username : STEAM_USER password : STEAM_PASS install_path : assetto executable_path : acServer force_update : false http : hostname : 0.0.0.0:8772 session_key : RANDOMLY_GENERATE_THIS server_manager_base_URL : session_store_type : cookie session_store_path : '' tls : enabled : false cert_path : '' key_path : '' monitoring : enabled : true store : type : boltdb path : server_manager.db shared_data_path : scheduled_event_check_loop : 0s accounts : admin_password_override : live_map : refresh_interval_ms : 500 server : audit_logging : true performance_mode : false dont_open_browser : false scan_content_folder_for_chanes : true use_car_name_cache : true persist_mid_session_results : false plugins : championships : recaptcha : site_key : secret_key : lua : enabled : false Note Make sure to replace STEAM_USER and STEAM_PASS with your steam account's information. I recommend you create a separate Steam account with Steam Guard disabled . You don't need an Assetto Corsa license to download the dedicated server. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8772 /tcp sudo ufw allow 9600 /udp sudo ufw allow 9600 /tcp sudo ufw allow 8081 /tcp Starting for the First Time \u00b6 Start up the game server with: docker-compose up -d Joining the Server \u00b6 If you have set the server to be LAN only, you may join your server by going to the following URL: https://acstuff.ru/s/q:race/online/join?ip=<IP>&httpPort=8081 Make sure the clients have AC Content Manager to be able to access through that URL.","title":"Assetto Corsa"},{"location":"linux/services/games/assettocorsa/#assetto-corsa","text":"Assetto Corsa is a realistic racing simulator. This game server has a community made server manager available on Docker Hub , however, I have made a small fork of this to update the source for SteamCMD since I've been having quite a lot of trouble getting it to work. This fork is available from GitHub Packages , which is the image we'll use for this.","title":"Assetto Corsa"},{"location":"linux/services/games/assettocorsa/#pre-installation","text":"We'll create a folder in the main user's home where all the game server's data will be saved. mkdir ~/games/assettocorsa","title":"Pre-Installation"},{"location":"linux/services/games/assettocorsa/#docker-compose","text":"The game server will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : assettocorsa : image : ghcr.io/moonstar-x/assetto-server-manager:master restart : unless-stopped user : 1000:1000 ports : - 8772:8772 - 9600:9600 - 9600:9600/udp - 8081:8081 volumes : - ./server:/home/assetto/server-manager/assetto - ./data:/home/assetto/server-manager Warning Make sure to create the data and server folder before starting the container, otherwise you'll have some problems with the server data being saved.","title":"Docker Compose"},{"location":"linux/services/games/assettocorsa/#configuration","text":"Create a config file inside data/config.yml : nano data/config.yml And paste the following: steam : username : STEAM_USER password : STEAM_PASS install_path : assetto executable_path : acServer force_update : false http : hostname : 0.0.0.0:8772 session_key : RANDOMLY_GENERATE_THIS server_manager_base_URL : session_store_type : cookie session_store_path : '' tls : enabled : false cert_path : '' key_path : '' monitoring : enabled : true store : type : boltdb path : server_manager.db shared_data_path : scheduled_event_check_loop : 0s accounts : admin_password_override : live_map : refresh_interval_ms : 500 server : audit_logging : true performance_mode : false dont_open_browser : false scan_content_folder_for_chanes : true use_car_name_cache : true persist_mid_session_results : false plugins : championships : recaptcha : site_key : secret_key : lua : enabled : false Note Make sure to replace STEAM_USER and STEAM_PASS with your steam account's information. I recommend you create a separate Steam account with Steam Guard disabled . You don't need an Assetto Corsa license to download the dedicated server.","title":"Configuration"},{"location":"linux/services/games/assettocorsa/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8772 /tcp sudo ufw allow 9600 /udp sudo ufw allow 9600 /tcp sudo ufw allow 8081 /tcp","title":"Post-Installation"},{"location":"linux/services/games/assettocorsa/#starting-for-the-first-time","text":"Start up the game server with: docker-compose up -d","title":"Starting for the First Time"},{"location":"linux/services/games/assettocorsa/#joining-the-server","text":"If you have set the server to be LAN only, you may join your server by going to the following URL: https://acstuff.ru/s/q:race/online/join?ip=<IP>&httpPort=8081 Make sure the clients have AC Content Manager to be able to access through that URL.","title":"Joining the Server"},{"location":"linux/services/games/teamspeak/","text":"TeamSpeak 3 \u00b6 TeamSpeak 3 is a gaming focused voice server. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/games/teamspeak Docker Compose \u00b6 The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : teamspeak : image : teamspeak:latest restart : unless-stopped depends_on : - db ports : - 9987:9987/udp - 10011:10011 - 30033:30033 volumes : - ./data:/var/ts3server environment : - TZ=America/Guayaquil - TS3SERVER_DB_PLUGIN=ts3db_mariadb - TS3SERVER_DB_SQLCREATEPATH=create_mariadb - TS3SERVER_DB_HOST=db - TS3SERVER_DB_USER=root - TS3SERVER_DB_NAME=teamspeak - TS3SERVER_DB_PASSWORD=CHANGE_THIS - TS3SERVER_DB_WAITUNTILREADY=30 - TS3SERVER_LICENSE=accept db : image : mariadb:10 restart : unless-stopped volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=teamspeak Note Make sure to change CHANGE_THIS to a custom value. Getting Server Auth Tokens \u00b6 After the container has been created, check its logs and save the serveradmin login details. This is very important in case you get locked out of your server or if you need to change some settings through ServerQuery. Use: docker logs teamspeak_teamspeak_1 --follow You should also find here the privilege key to set up your user as the server administrator. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 10011 /tcp sudo ufw allow 30033 /tcp sudo ufw allow 9987 /udp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"TeamSpeak 3"},{"location":"linux/services/games/teamspeak/#teamspeak-3","text":"TeamSpeak 3 is a gaming focused voice server. This service has an official image available on Docker Hub which we'll use.","title":"TeamSpeak 3"},{"location":"linux/services/games/teamspeak/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/games/teamspeak","title":"Pre-Installation"},{"location":"linux/services/games/teamspeak/#docker-compose","text":"The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : teamspeak : image : teamspeak:latest restart : unless-stopped depends_on : - db ports : - 9987:9987/udp - 10011:10011 - 30033:30033 volumes : - ./data:/var/ts3server environment : - TZ=America/Guayaquil - TS3SERVER_DB_PLUGIN=ts3db_mariadb - TS3SERVER_DB_SQLCREATEPATH=create_mariadb - TS3SERVER_DB_HOST=db - TS3SERVER_DB_USER=root - TS3SERVER_DB_NAME=teamspeak - TS3SERVER_DB_PASSWORD=CHANGE_THIS - TS3SERVER_DB_WAITUNTILREADY=30 - TS3SERVER_LICENSE=accept db : image : mariadb:10 restart : unless-stopped volumes : - ./db:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=CHANGE_THIS - MYSQL_DATABASE=teamspeak Note Make sure to change CHANGE_THIS to a custom value.","title":"Docker Compose"},{"location":"linux/services/games/teamspeak/#getting-server-auth-tokens","text":"After the container has been created, check its logs and save the serveradmin login details. This is very important in case you get locked out of your server or if you need to change some settings through ServerQuery. Use: docker logs teamspeak_teamspeak_1 --follow You should also find here the privilege key to set up your user as the server administrator.","title":"Getting Server Auth Tokens"},{"location":"linux/services/games/teamspeak/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 10011 /tcp sudo ufw allow 30033 /tcp sudo ufw allow 9987 /udp","title":"Post-Installation"},{"location":"linux/services/games/teamspeak/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/games/zerotier/","text":"ZeroTier-One \u00b6 ZeroTier-One is a virtual LAN service, similar to Hamachi, that allows you to have your services exposed through a VPN. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/games/zerotier Creating a Network \u00b6 To create a network, simply visit My ZeroTier , login to your account (or create one if needed) and simply click on the Create Network button. This will give you a Network ID (which you should keep since we'll need this). This Network ID is what you need to share with your friends so that they can connect to your network. If you leave the network settings to be private, you may need to manually authorize new members into the network. Docker Compose \u00b6 The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : zerotier : image : zerotier/zerotier:latest restart : unless-stopped cap_add : - NET_ADMIN network_mode : host devices : - /dev/net/tun:/dev/net/tun command : NETWORK_ID volumes : - ./config/authtoken.secret:/var/lib/zerotier-one/authtoken.secret - ./config/identity.public:/var/lib/zerotier-one/identity.public - ./config/identity.secret:/var/lib/zerotier-one/identity.secret environment : - TZ=America/Guayaquil dns : image : zerotier/zeronsd:latest restart : unless-stopped depends_on : - zerotier network_mode : host command : start NETWORK_ID -d DOMAIN volumes : - ./config/authtoken.secret:/var/lib/zerotier-one/authtoken.secret:ro environment : - TZ=America/Guayaquil - ZEROTIER_CENTRAL_TOKEN=CENTRAL_TOKEN Note Replace NETWORK_ID with your Network ID . Replace CENTRAL_TOKEN with a ZeroTier token, you may acquire one from here . Replace DOMAIN with the domain you wish to use. Note that the domain only represents the base domain, everyone in your network will be assigned a subdomain based on their assigned names on the network. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 53 /tcp sudo ufw allow 53 /udp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"ZeroTier-One"},{"location":"linux/services/games/zerotier/#zerotier-one","text":"ZeroTier-One is a virtual LAN service, similar to Hamachi, that allows you to have your services exposed through a VPN. This service has an official image available on Docker Hub which we'll use.","title":"ZeroTier-One"},{"location":"linux/services/games/zerotier/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/games/zerotier","title":"Pre-Installation"},{"location":"linux/services/games/zerotier/#creating-a-network","text":"To create a network, simply visit My ZeroTier , login to your account (or create one if needed) and simply click on the Create Network button. This will give you a Network ID (which you should keep since we'll need this). This Network ID is what you need to share with your friends so that they can connect to your network. If you leave the network settings to be private, you may need to manually authorize new members into the network.","title":"Creating a Network"},{"location":"linux/services/games/zerotier/#docker-compose","text":"The bot will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : zerotier : image : zerotier/zerotier:latest restart : unless-stopped cap_add : - NET_ADMIN network_mode : host devices : - /dev/net/tun:/dev/net/tun command : NETWORK_ID volumes : - ./config/authtoken.secret:/var/lib/zerotier-one/authtoken.secret - ./config/identity.public:/var/lib/zerotier-one/identity.public - ./config/identity.secret:/var/lib/zerotier-one/identity.secret environment : - TZ=America/Guayaquil dns : image : zerotier/zeronsd:latest restart : unless-stopped depends_on : - zerotier network_mode : host command : start NETWORK_ID -d DOMAIN volumes : - ./config/authtoken.secret:/var/lib/zerotier-one/authtoken.secret:ro environment : - TZ=America/Guayaquil - ZEROTIER_CENTRAL_TOKEN=CENTRAL_TOKEN Note Replace NETWORK_ID with your Network ID . Replace CENTRAL_TOKEN with a ZeroTier token, you may acquire one from here . Replace DOMAIN with the domain you wish to use. Note that the domain only represents the base domain, everyone in your network will be assigned a subdomain based on their assigned names on the network.","title":"Docker Compose"},{"location":"linux/services/games/zerotier/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 53 /tcp sudo ufw allow 53 /udp","title":"Post-Installation"},{"location":"linux/services/games/zerotier/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to media servers. mkdir ~/media For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/media/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to media servers. mkdir ~/media For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/media/freshrss/","text":"FreshRSS \u00b6 FreshRSS is an RSS feed reader. For this service we'll use an image from LinuxServer available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/freshrss Docker Compose \u00b6 FreshRSS will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : freshrss : image : lscr.io/linuxserver/freshrss:latest restart : unless-stopped ports : - 5200:80 volumes : - ./data:/config environment : - TZ=America/Guayaquil - PUID=1000 - PGID=1000 Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 5200 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"FreshRSS"},{"location":"linux/services/media/freshrss/#freshrss","text":"FreshRSS is an RSS feed reader. For this service we'll use an image from LinuxServer available on Docker Hub which we'll use.","title":"FreshRSS"},{"location":"linux/services/media/freshrss/#pre-installation","text":"We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/freshrss","title":"Pre-Installation"},{"location":"linux/services/media/freshrss/#docker-compose","text":"FreshRSS will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : freshrss : image : lscr.io/linuxserver/freshrss:latest restart : unless-stopped ports : - 5200:80 volumes : - ./data:/config environment : - TZ=America/Guayaquil - PUID=1000 - PGID=1000 Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami .","title":"Docker Compose"},{"location":"linux/services/media/freshrss/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 5200 /tcp","title":"Post-Installation"},{"location":"linux/services/media/freshrss/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/jellyfin-embystat/","text":"Jellyfin + EmbyStat \u00b6 Note This is a multi-service stack. Jellyfin is an open source version of the famous media server Emby . This media server has an official image available on Docker Hub which we'll use. EmbyStat is an analytics service for Jellyfin/Emby. There is no official docker image for EmbyStat , however we'll use one from LinuxServer on Docker Hub . Pre-Installation \u00b6 We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/jellyfin Docker Compose \u00b6 Jellyfin + EmbyStat will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : jellyfin : image : jellyfin/jellyfin:latest user : 1000:1000 restart : unless-stopped volumes : - ./jellyfin-config:/config - ./jellyfin-cache:/cache - /media/usb_4tb_2/Jellyfin:/media ports : - 8096:8096 environment : - TZ=America/Guayaquil embystat : image : ghcr.io/linuxserver/embystat:latest restart : unless-stopped depends_on : - jellyfin volumes : - ./embystat-config:/config ports : - 6555:6555 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil Note In the case of the user directive, 1000:1000 corresponds to the user's UID:GID . You can find the values for your own user by running id $whoami . Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8086 /tcp sudo ufw allow 6555 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Jellyfin + EmbyStat"},{"location":"linux/services/media/jellyfin-embystat/#jellyfin-embystat","text":"Note This is a multi-service stack. Jellyfin is an open source version of the famous media server Emby . This media server has an official image available on Docker Hub which we'll use. EmbyStat is an analytics service for Jellyfin/Emby. There is no official docker image for EmbyStat , however we'll use one from LinuxServer on Docker Hub .","title":"Jellyfin + EmbyStat"},{"location":"linux/services/media/jellyfin-embystat/#pre-installation","text":"We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/jellyfin","title":"Pre-Installation"},{"location":"linux/services/media/jellyfin-embystat/#docker-compose","text":"Jellyfin + EmbyStat will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : jellyfin : image : jellyfin/jellyfin:latest user : 1000:1000 restart : unless-stopped volumes : - ./jellyfin-config:/config - ./jellyfin-cache:/cache - /media/usb_4tb_2/Jellyfin:/media ports : - 8096:8096 environment : - TZ=America/Guayaquil embystat : image : ghcr.io/linuxserver/embystat:latest restart : unless-stopped depends_on : - jellyfin volumes : - ./embystat-config:/config ports : - 6555:6555 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil Note In the case of the user directive, 1000:1000 corresponds to the user's UID:GID . You can find the values for your own user by running id $whoami . Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami .","title":"Docker Compose"},{"location":"linux/services/media/jellyfin-embystat/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8086 /tcp sudo ufw allow 6555 /tcp","title":"Post-Installation"},{"location":"linux/services/media/jellyfin-embystat/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/kavita/","text":"Kavita \u00b6 Kavita is an eBook server for PDFs and EPUBs. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/kavita Docker Compose \u00b6 Kavita will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : kavita : image : kizaing/kavita:latest restart : unless-stopped ports : - 5000:5000 volumes : - ./data:/kavita/config - /media/sata_2tb/Books:/books environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 5000 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Kavita"},{"location":"linux/services/media/kavita/#kavita","text":"Kavita is an eBook server for PDFs and EPUBs. This service has an official image available on Docker Hub which we'll use.","title":"Kavita"},{"location":"linux/services/media/kavita/#pre-installation","text":"We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/kavita","title":"Pre-Installation"},{"location":"linux/services/media/kavita/#docker-compose","text":"Kavita will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : kavita : image : kizaing/kavita:latest restart : unless-stopped ports : - 5000:5000 volumes : - ./data:/kavita/config - /media/sata_2tb/Books:/books environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/media/kavita/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 5000 /tcp","title":"Post-Installation"},{"location":"linux/services/media/kavita/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/media-acquisition/","text":"Media Acquisition \u00b6 Note This is a multi-service stack. Transmission is a BitTorrent client. There is no official docker image for Transmission , however we'll use one from LinuxServer on Docker Hub . JDownloader is a download client that makes downloading from direct links a breeze. There is no official image for JDownloader , however we'll use one available on Docker Hub . Ombi is a media request tracker, useful for when you share a Plex server or similar with family and friends. There is no official docker image for Ombi , however we'll use one from LinuxServer on Docker Hub . Sonarr is an RSS downloader focused on TV Shows. There is no official docker image for Sonarr , however we'll use one from LinuxServer on Docker Hub . Radarr is an RSS downloader focused on movies. There is no official docker image for Radarr , however we'll use one from LinuxServer on Docker Hub . Lidarr is an RSS downloader focused on music. For this service we will use a fork that comes integrated with Deemix which allows free download of music from Deezer. The image is available on Docker Hub . Bazarr is an RSS downloader focused on subtitles. There is no official docker image for Bazarr , however we'll use one from LinuxServer on Docker Hub . Readarr is an RSS downloader focused on ebooks and audiobooks. There is no official docker image for Readarr , however we'll use one from LinuxServer on Docker Hub . Prowlarr is an indexer for -arr software. There is no official docker image for Prowlarr , however we'll use one from LinuxServer on Docker Hub . OpenBooks is an ebook downloader that can download from IRC Highway. There is an official docker image for OpenBooks GitHub Packages which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/media/media_acquisition We'll need to download a GUI for Transmission manually. For this, inside the newly created folder, run the following commands: curl -OL https://github.com/johman10/flood-for-transmission/releases/download/latest/flood-for-transmission.zip unzip flood-for-transmission.zip && rm flood-for-transmission.zip mv flood-for-transmission transmission-web-ui Docker Compose \u00b6 Media Acquisition will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : transmission : image : ghcr.io/linuxserver/transmission:latest restart : unless-stopped volumes : - ./transmission-config:/config - ./transmission-watch:/watch - ./transmission-web-ui:/flood-for-transmission - /media/sata_2tb/Downloads:/downloads ports : - 9091:9091 - 51413:51413 - 51413:51413/udp environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil - TRANSMISSION_WEB_HOME=/flood-for-transmission/ - USER=<USER_HERE> - PASS=<PASS_HERE> jdownloader : image : jlesage/jdownloader-2:latest restart : unless-stopped volumes : - ./jdownloader-config:/config - /media/sata_2tb/Downloads:/output ports : - 3129:3129 - 5800:5800 environment : - TZ=America/Guayaquil - USER_ID=1000 - GROUP_ID=1000 - VNC_PASSWORD=<PASS_HERE> ombi : image : ghcr.io/linuxserver/ombi:latest restart : unless-stopped volumes : - ./ombi-config:/config ports : - 3579:3579 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil sonarr : image : ghcr.io/linuxserver/sonarr:latest restart : unless-stopped volumes : - ./sonarr-config:/config - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb - /media/sata_2tb/Downloads:/downloads ports : - 8989:8989 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil radarr : image : ghcr.io/linuxserver/radarr:latest restart : unless-stopped volumes : - ./radarr-config:/config - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb - /media/sata_2tb/Downloads:/downloads ports : - 7878:7878 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil lidarr : image : youegraillot/lidarr-on-steroids:latest restart : unless-stopped volumes : - ./lidarr-config:/config - ./deemix-config:/config_deemix - /media/sata_2tb/Plex Music:/music - /media/sata_2tb/Deemix Downloads:/downloads ports : - 8686:8686 - 6595:6595 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil - AUTOCONFIG=true bazarr : image : ghcr.io/linuxserver/bazarr:latest restart : unless-stopped volumes : - ./bazarr-config:/config - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb - /media/sata_2tb/Downloads:/downloads ports : - 6767:6767 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil readarr : image : ghcr.io/linuxserver/readarr:develop restart : unless-stopped volumes : - ./readarr-config:/config - /media/sata_2tb/Books:/books - /media/sata_2tb/Downloads:/downloads ports : - 8787:8787 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil prowlarr : image : ghcr.io/linuxserver/prowlarr:latest restart : unless-stopped volumes : - ./prowlarr-config:/config ports : - 9696:9696 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil openbooks : image : evanbuss/openbooks:latest restart : unless-stopped volumes : - /media/sata_2tb/Books/OpenBooks:/books ports : - 12450:80 command : --name <USER_HERE> --persist environment : - TZ=America/Guayaquil Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note The password should be set inside the docker-compose.yml file and not manually updated in transmission-config/settings.json which will mess up with the s6 supervisor. Use a password you don't care too much about since it would basically be saved in plain text. Note You may change the contents of transmission-config/settings.json as long as the container is stopped. Note In the case of the USER_ID and GROUP_ID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Post-Installation \u00b6 We'll need to allow the service's web UI port on our firewall. sudo ufw allow 9091 /tcp sudo ufw allow 3129 /tcp sudo ufw allow 5800 /tcp sudo ufw allow 3579 /tcp sudo ufw allow 8989 /tcp sudo ufw allow 7878 /tcp sudo ufw allow 8686 /tcp sudo ufw allow 6595 /tcp sudo ufw allow 6767 /tcp sudo ufw allow 8787 /tcp sudo ufw allow 9696 /tcp sudo ufw allow 12450 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Media Acquisition"},{"location":"linux/services/media/media-acquisition/#media-acquisition","text":"Note This is a multi-service stack. Transmission is a BitTorrent client. There is no official docker image for Transmission , however we'll use one from LinuxServer on Docker Hub . JDownloader is a download client that makes downloading from direct links a breeze. There is no official image for JDownloader , however we'll use one available on Docker Hub . Ombi is a media request tracker, useful for when you share a Plex server or similar with family and friends. There is no official docker image for Ombi , however we'll use one from LinuxServer on Docker Hub . Sonarr is an RSS downloader focused on TV Shows. There is no official docker image for Sonarr , however we'll use one from LinuxServer on Docker Hub . Radarr is an RSS downloader focused on movies. There is no official docker image for Radarr , however we'll use one from LinuxServer on Docker Hub . Lidarr is an RSS downloader focused on music. For this service we will use a fork that comes integrated with Deemix which allows free download of music from Deezer. The image is available on Docker Hub . Bazarr is an RSS downloader focused on subtitles. There is no official docker image for Bazarr , however we'll use one from LinuxServer on Docker Hub . Readarr is an RSS downloader focused on ebooks and audiobooks. There is no official docker image for Readarr , however we'll use one from LinuxServer on Docker Hub . Prowlarr is an indexer for -arr software. There is no official docker image for Prowlarr , however we'll use one from LinuxServer on Docker Hub . OpenBooks is an ebook downloader that can download from IRC Highway. There is an official docker image for OpenBooks GitHub Packages which we'll use.","title":"Media Acquisition"},{"location":"linux/services/media/media-acquisition/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/media/media_acquisition We'll need to download a GUI for Transmission manually. For this, inside the newly created folder, run the following commands: curl -OL https://github.com/johman10/flood-for-transmission/releases/download/latest/flood-for-transmission.zip unzip flood-for-transmission.zip && rm flood-for-transmission.zip mv flood-for-transmission transmission-web-ui","title":"Pre-Installation"},{"location":"linux/services/media/media-acquisition/#docker-compose","text":"Media Acquisition will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : transmission : image : ghcr.io/linuxserver/transmission:latest restart : unless-stopped volumes : - ./transmission-config:/config - ./transmission-watch:/watch - ./transmission-web-ui:/flood-for-transmission - /media/sata_2tb/Downloads:/downloads ports : - 9091:9091 - 51413:51413 - 51413:51413/udp environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil - TRANSMISSION_WEB_HOME=/flood-for-transmission/ - USER=<USER_HERE> - PASS=<PASS_HERE> jdownloader : image : jlesage/jdownloader-2:latest restart : unless-stopped volumes : - ./jdownloader-config:/config - /media/sata_2tb/Downloads:/output ports : - 3129:3129 - 5800:5800 environment : - TZ=America/Guayaquil - USER_ID=1000 - GROUP_ID=1000 - VNC_PASSWORD=<PASS_HERE> ombi : image : ghcr.io/linuxserver/ombi:latest restart : unless-stopped volumes : - ./ombi-config:/config ports : - 3579:3579 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil sonarr : image : ghcr.io/linuxserver/sonarr:latest restart : unless-stopped volumes : - ./sonarr-config:/config - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb - /media/sata_2tb/Downloads:/downloads ports : - 8989:8989 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil radarr : image : ghcr.io/linuxserver/radarr:latest restart : unless-stopped volumes : - ./radarr-config:/config - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb - /media/sata_2tb/Downloads:/downloads ports : - 7878:7878 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil lidarr : image : youegraillot/lidarr-on-steroids:latest restart : unless-stopped volumes : - ./lidarr-config:/config - ./deemix-config:/config_deemix - /media/sata_2tb/Plex Music:/music - /media/sata_2tb/Deemix Downloads:/downloads ports : - 8686:8686 - 6595:6595 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil - AUTOCONFIG=true bazarr : image : ghcr.io/linuxserver/bazarr:latest restart : unless-stopped volumes : - ./bazarr-config:/config - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb - /media/sata_2tb/Downloads:/downloads ports : - 6767:6767 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil readarr : image : ghcr.io/linuxserver/readarr:develop restart : unless-stopped volumes : - ./readarr-config:/config - /media/sata_2tb/Books:/books - /media/sata_2tb/Downloads:/downloads ports : - 8787:8787 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil prowlarr : image : ghcr.io/linuxserver/prowlarr:latest restart : unless-stopped volumes : - ./prowlarr-config:/config ports : - 9696:9696 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil openbooks : image : evanbuss/openbooks:latest restart : unless-stopped volumes : - /media/sata_2tb/Books/OpenBooks:/books ports : - 12450:80 command : --name <USER_HERE> --persist environment : - TZ=America/Guayaquil Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note The password should be set inside the docker-compose.yml file and not manually updated in transmission-config/settings.json which will mess up with the s6 supervisor. Use a password you don't care too much about since it would basically be saved in plain text. Note You may change the contents of transmission-config/settings.json as long as the container is stopped. Note In the case of the USER_ID and GROUP_ID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami .","title":"Docker Compose"},{"location":"linux/services/media/media-acquisition/#post-installation","text":"We'll need to allow the service's web UI port on our firewall. sudo ufw allow 9091 /tcp sudo ufw allow 3129 /tcp sudo ufw allow 5800 /tcp sudo ufw allow 3579 /tcp sudo ufw allow 8989 /tcp sudo ufw allow 7878 /tcp sudo ufw allow 8686 /tcp sudo ufw allow 6595 /tcp sudo ufw allow 6767 /tcp sudo ufw allow 8787 /tcp sudo ufw allow 9696 /tcp sudo ufw allow 12450 /tcp","title":"Post-Installation"},{"location":"linux/services/media/media-acquisition/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/pigallery-2/","text":"PiGallery 2 \u00b6 PiGallery 2 is a simple image gallery. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/pigallery2 Docker Compose \u00b6 PiGallery 2 will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : pigallery2 : image : bpatrik/pigallery2:latest restart : unless-stopped volumes : - ./config:/app/data/config - ./data:/app/data/db - /media/usb_4tb_2/Gallery/cache:/app/data/tmp - /media/usb_4tb_2/Gallery/Images:/app/data/images:ro ports : - 20500:80 environment : - TZ=America/Guayaquil - NODE_ENV=productio Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 20500 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"PiGallery 2"},{"location":"linux/services/media/pigallery-2/#pigallery-2","text":"PiGallery 2 is a simple image gallery. This service has an official image available on Docker Hub which we'll use.","title":"PiGallery 2"},{"location":"linux/services/media/pigallery-2/#pre-installation","text":"We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/pigallery2","title":"Pre-Installation"},{"location":"linux/services/media/pigallery-2/#docker-compose","text":"PiGallery 2 will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : pigallery2 : image : bpatrik/pigallery2:latest restart : unless-stopped volumes : - ./config:/app/data/config - ./data:/app/data/db - /media/usb_4tb_2/Gallery/cache:/app/data/tmp - /media/usb_4tb_2/Gallery/Images:/app/data/images:ro ports : - 20500:80 environment : - TZ=America/Guayaquil - NODE_ENV=productio","title":"Docker Compose"},{"location":"linux/services/media/pigallery-2/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 20500 /tcp","title":"Post-Installation"},{"location":"linux/services/media/pigallery-2/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/plex-tautulli-synclounge/","text":"Plex + Tautulli + Synclounge \u00b6 Note This is a multi-service stack. Plex is one of the most popular media server options out there. This media server has an official image available on Docker Hub which we'll use. Tautulli is a monitoring tool for Plex . This service has an official image available on Docker Hub which we'll use. Synclounge is a tool that lets your Plex users watch something simultaneously. There is no official docker image for Synclounge , however we'll use one from LinuxServer on Docker Hub . Pre-Installation \u00b6 We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/plex Docker Compose \u00b6 Plex + Tautulli + Synclounge will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : plex : image : plexinc/pms-docker:latest restart : unless-stopped network_mode : host volumes : - ./plex-config:/config - ./plex-transcode:/transcode - /media/usb_1tb:/media/usb_1tb - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb environment : - TZ=America/Guayaquil - PLEX_UID=1000 - PLEX_GID=1000 tautulli : image : tautulli/tautulli:latest restart : unless-stopped depends_on : - plex volumes : - ./tautulli-config:/config ports : - 8181:8181 environment : - TZ=America/Guayaquil - PUID=1000 - PGID=1000 synclounge : image : ghcr.io/linuxserver/synclounge:latest restart : unless-stopped depends_on : - plex ports : - 8088:8088 environment : - TZ=America/Guayaquil Note In the case of the PLEX_UID and PLEX_GID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 32400 /tcp sudo ufw allow 8181 /tcp sudo ufw allow 8088 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Plex + Tautulli + Synclounge"},{"location":"linux/services/media/plex-tautulli-synclounge/#plex-tautulli-synclounge","text":"Note This is a multi-service stack. Plex is one of the most popular media server options out there. This media server has an official image available on Docker Hub which we'll use. Tautulli is a monitoring tool for Plex . This service has an official image available on Docker Hub which we'll use. Synclounge is a tool that lets your Plex users watch something simultaneously. There is no official docker image for Synclounge , however we'll use one from LinuxServer on Docker Hub .","title":"Plex + Tautulli + Synclounge"},{"location":"linux/services/media/plex-tautulli-synclounge/#pre-installation","text":"We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/plex","title":"Pre-Installation"},{"location":"linux/services/media/plex-tautulli-synclounge/#docker-compose","text":"Plex + Tautulli + Synclounge will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : plex : image : plexinc/pms-docker:latest restart : unless-stopped network_mode : host volumes : - ./plex-config:/config - ./plex-transcode:/transcode - /media/usb_1tb:/media/usb_1tb - /media/usb_4tb:/media/usb_4tb - /media/usb_8tb:/media/usb_8tb environment : - TZ=America/Guayaquil - PLEX_UID=1000 - PLEX_GID=1000 tautulli : image : tautulli/tautulli:latest restart : unless-stopped depends_on : - plex volumes : - ./tautulli-config:/config ports : - 8181:8181 environment : - TZ=America/Guayaquil - PUID=1000 - PGID=1000 synclounge : image : ghcr.io/linuxserver/synclounge:latest restart : unless-stopped depends_on : - plex ports : - 8088:8088 environment : - TZ=America/Guayaquil Note In the case of the PLEX_UID and PLEX_GID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami .","title":"Docker Compose"},{"location":"linux/services/media/plex-tautulli-synclounge/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 32400 /tcp sudo ufw allow 8181 /tcp sudo ufw allow 8088 /tcp","title":"Post-Installation"},{"location":"linux/services/media/plex-tautulli-synclounge/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/media/reddit-slideshow/","text":"Reddit Slideshow \u00b6 Reddit Slideshow is a web app that can render image posts from Reddit as a slideshow. I made a little fork of the original repo and added a Docker image which can be accessed through my Git server . The fork includes a little landing page with some buttons to quickly access to specific slideshows. These buttons can be configured through a settings.json file which will be shown later on. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/reddit-slideshow Docker Compose \u00b6 Reddit Slideshow will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : web : image : code.moonstar-x.dev/public/reddit-slideshow:latest restart : unless-stopped volumes : - ./settings.json:/usr/share/nginx/html/settings.json ports : - 26969:8080 environment : - TZ=America/Guayaquil Create a settings.json file and add something like this: { \"slideshows\" : [ \"funny\" , [ \"memes\" , \"pics\" ] ] } You can add the subreddits you want as a string for single subreddits or as an array for multireddit. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 26969 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Reddit Slideshow"},{"location":"linux/services/media/reddit-slideshow/#reddit-slideshow","text":"Reddit Slideshow is a web app that can render image posts from Reddit as a slideshow. I made a little fork of the original repo and added a Docker image which can be accessed through my Git server . The fork includes a little landing page with some buttons to quickly access to specific slideshows. These buttons can be configured through a settings.json file which will be shown later on.","title":"Reddit Slideshow"},{"location":"linux/services/media/reddit-slideshow/#pre-installation","text":"We'll create a folder in the main user's home where all the media server's data will be saved. mkdir ~/media/reddit-slideshow","title":"Pre-Installation"},{"location":"linux/services/media/reddit-slideshow/#docker-compose","text":"Reddit Slideshow will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : web : image : code.moonstar-x.dev/public/reddit-slideshow:latest restart : unless-stopped volumes : - ./settings.json:/usr/share/nginx/html/settings.json ports : - 26969:8080 environment : - TZ=America/Guayaquil Create a settings.json file and add something like this: { \"slideshows\" : [ \"funny\" , [ \"memes\" , \"pics\" ] ] } You can add the subreddits you want as a string for single subreddits or as an array for multireddit.","title":"Docker Compose"},{"location":"linux/services/media/reddit-slideshow/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 26969 /tcp","title":"Post-Installation"},{"location":"linux/services/media/reddit-slideshow/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/monitoring/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to system monitoring related services. mkdir ~/monitoring For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/monitoring/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to system monitoring related services. mkdir ~/monitoring For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/monitoring/grafana/","text":"Grafana \u00b6 Warning You should set up Prometheus first before setting this up. Grafana is a dashboard that can consume from Prometheus and other data sources to display all the information that you need. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/grafana Inside this folder, create a data folder: mkdir data Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : grafana : image : grafana/grafana:latest restart : unless-stopped user : 1000:1000 ports : - 33000:3000 volumes : - ./data:/var/lib/grafana environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 33000 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure. Add Prometheus to Grafana \u00b6 Visit the Grafana's site by accessing: http://localhost:3000 And create your Admin account. Make your way to the following page: Configuration > Data Sources > Add a data source > Prometheus And add the local IP of your server. Add a Dashboard \u00b6 Make your way to the following page. Create > Import And paste the following JSON in it: { \"annotations\" : { \"list\" : [ { \"builtIn\" : 1 , \"datasource\" : { \"type\" : \"grafana\" , \"uid\" : \"-- Grafana --\" }, \"enable\" : true , \"hide\" : true , \"iconColor\" : \"rgba(0, 211, 255, 1)\" , \"name\" : \"Annotations & Alerts\" , \"target\" : { \"limit\" : 100 , \"matchAny\" : false , \"tags\" : [], \"type\" : \"dashboard\" }, \"type\" : \"dashboard\" } ] }, \"description\" : \"moonstar's Server Dashboard\" , \"editable\" : true , \"fiscalYearStartMonth\" : 0 , \"graphTooltip\" : 0 , \"id\" : 4 , \"iteration\" : 1653438285360 , \"links\" : [], \"liveNow\" : false , \"panels\" : [ { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 0 }, \"id\" : 8 , \"panels\" : [], \"title\" : \"Quick Info\" , \"type\" : \"row\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 75 }, { \"color\" : \"semi-dark-red\" , \"value\" : 95 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 0 , \"y\" : 1 }, \"id\" : 2 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"(((count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu))) - avg(sum by (mode)(irate(node_cpu_seconds_total{mode='idle',instance=\\\"$node\\\",job=\\\"$job\\\"}[5m])))) * 100) / count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu))\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Usage\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU temperature\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"blue\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 45 }, { \"color\" : \"semi-dark-red\" , \"value\" : 65 } ] }, \"unit\" : \"celsius\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 3 , \"y\" : 1 }, \"id\" : 5 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"mean\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(node_hwmon_temp_celsius{instance=\\\"$node\\\",job=\\\"$job\\\"})\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Temp\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Sys Load (5m avg)\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 75 }, { \"color\" : \"semi-dark-red\" , \"value\" : 95 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 6 , \"y\" : 1 }, \"id\" : 6 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(node_load5{instance=\\\"$node\\\",job=\\\"$job\\\"}) / count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu)) * 100\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Sys Load\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"RAM Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 70 }, { \"color\" : \"semi-dark-red\" , \"value\" : 90 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 9 , \"y\" : 1 }, \"id\" : 3 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"100 - ((node_memory_MemAvailable_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"} * 100) / node_memory_MemTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"})\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"RAM Usage\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Swap Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 15 }, { \"color\" : \"semi-dark-red\" , \"value\" : 30 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 12 , \"y\" : 1 }, \"id\" : 4 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"((node_memory_SwapTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"} - node_memory_SwapFree_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"}) / (node_memory_SwapTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"} )) * 100\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Swap Usage\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Cores\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"short\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 15 , \"y\" : 1 }, \"id\" : 10 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu))\" , \"refId\" : \"A\" } ], \"title\" : \"CPU Cores\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"RAM Total\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 18 , \"y\" : 1 }, \"id\" : 11 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_memory_MemTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"RAM Total\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Swap Total\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 21 , \"y\" : 1 }, \"id\" : 12 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_memory_SwapTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"Swap Total\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Uptime\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"dtdhms\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 15 , \"y\" : 3 }, \"id\" : 13 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_time_seconds{instance=\\\"$node\\\",job=\\\"$job\\\"} - node_boot_time_seconds{instance=\\\"$node\\\",job=\\\"$job\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"Uptime\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"/ Total Size\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 18 , \"y\" : 3 }, \"id\" : 14 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_filesystem_size_bytes{instance=\\\"$node\\\",job=\\\"$job\\\",mountpoint=\\\"/\\\",fstype!=\\\"rootfs\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"/ Total Size\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"/ Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 65 }, { \"color\" : \"semi-dark-red\" , \"value\" : 90 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 21 , \"y\" : 3 }, \"id\" : 15 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"100 - ((node_filesystem_avail_bytes{instance=\\\"$node\\\",job=\\\"$job\\\",mountpoint=\\\"/\\\",fstype!=\\\"rootfs\\\"} * 100) / node_filesystem_size_bytes{instance=\\\"$node\\\",job=\\\"$job\\\",mountpoint=\\\"/\\\",fstype!=\\\"rootfs\\\"})\" , \"refId\" : \"A\" } ], \"title\" : \"/ Usage\" , \"type\" : \"stat\" }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 5 }, \"id\" : 26 , \"panels\" : [], \"title\" : \"Network Stats\" , \"type\" : \"row\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Bytes Received/Transferred [$dev]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 8 , \"x\" : 0 , \"y\" : 6 }, \"id\" : 28 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"node_network_receive_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}\" , \"legendFormat\" : \"Bytes Received\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"node_network_transmit_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}\" , \"hide\" : false , \"legendFormat\" : \"Bytes Transferred\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Bytes Received/Transferred\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Transfer Speed [$network_interface]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"Bps\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 8 , \"x\" : 8 , \"y\" : 6 }, \"id\" : 29 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"rate(node_network_receive_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}[5m])\" , \"instant\" : false , \"legendFormat\" : \"Receive Speed\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"rate(node_network_transmit_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}[5m])\" , \"hide\" : false , \"legendFormat\" : \"Transmit Speed\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Transfer Speed\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"TCP Connections [$network_interface]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"short\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 8 , \"x\" : 16 , \"y\" : 6 }, \"id\" : 30 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"node_netstat_Tcp_CurrEstab\" , \"instant\" : false , \"legendFormat\" : \"Number of Connections\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"TCP Connections\" , \"type\" : \"timeseries\" }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 14 }, \"id\" : 32 , \"panels\" : [], \"title\" : \"CPU Stats\" , \"type\" : \"row\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Core Frequency\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"rothz\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 11 , \"w\" : 8 , \"x\" : 0 , \"y\" : 15 }, \"id\" : 34 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"node_cpu_scaling_frequency_hertz\" , \"legendFormat\" : \"Core {{cpu}}\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Core Frequency\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Usage by Mode\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 11 , \"w\" : 8 , \"x\" : 8 , \"y\" : 15 }, \"id\" : 35 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" , \"sortBy\" : \"Max\" , \"sortDesc\" : true }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"system\\\"}[5m]) * 100) \" , \"instant\" : false , \"legendFormat\" : \"system\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"user\\\"}[5m]) * 100) \" , \"hide\" : false , \"legendFormat\" : \"user\" , \"range\" : true , \"refId\" : \"B\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"iowait\\\"}[5m]) * 100) \" , \"hide\" : false , \"legendFormat\" : \"iowait\" , \"range\" : true , \"refId\" : \"C\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"steal\\\"}[5m]) * 100) \" , \"hide\" : false , \"legendFormat\" : \"steal\" , \"range\" : true , \"refId\" : \"D\" } ], \"title\" : \"CPU Usage by Mode\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 11 , \"w\" : 8 , \"x\" : 16 , \"y\" : 15 }, \"id\" : 36 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" , \"sortBy\" : \"Max\" , \"sortDesc\" : true }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"system\\\"}[5m]) * 100) + avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"user\\\"}[5m]) * 100) + avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"iowait\\\"}[5m]) * 100) + avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"steal\\\"}[5m]) * 100) \" , \"instant\" : false , \"legendFormat\" : \"CPU Usage\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Usage\" , \"type\" : \"timeseries\" }, { \"collapsed\" : true , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 26 }, \"id\" : 17 , \"panels\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Bytes Read/Written [$drive_name ]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : true , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 9 , \"x\" : 0 , \"y\" : 27 }, \"id\" : 19 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"rate(node_disk_read_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[5m])\" , \"instant\" : false , \"legendFormat\" : \"Bytes Read\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"rate(node_disk_written_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[5m])\" , \"hide\" : false , \"legendFormat\" : \"Bytes Written\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Bytes Read/Written\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Disk IOps Completed [$drive_name ]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : true , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" } ] }, \"unit\" : \"iops\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 9 , \"x\" : 9 , \"y\" : 27 }, \"id\" : 24 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"irate(node_disk_reads_completed_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[1m])\" , \"instant\" : false , \"legendFormat\" : \"Reads Completed\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"irate(node_disk_writes_completed_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[1m])\" , \"hide\" : false , \"legendFormat\" : \"Writes Completed\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Disk IOps Completed\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Total Size\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 18 , \"y\" : 27 }, \"id\" : 20 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"})\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Total Size\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Used Space\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"yellow\" } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 21 , \"y\" : 27 }, \"id\" : 22 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"}) - sum(node_filesystem_avail_bytes{device=~\\\".+$drive_name.?\\\"})\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Used Space\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Free Space\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"semi-dark-red\" }, { \"color\" : \"#EAB839\" , \"value\" : 20 }, { \"color\" : \"green\" , \"value\" : 50 } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 18 , \"y\" : 31 }, \"id\" : 21 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"sum(node_filesystem_avail_bytes{device=~\\\".+$drive_name.?\\\"})\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Free Space\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Disk Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" }, { \"color\" : \"#EAB839\" , \"value\" : 75 }, { \"color\" : \"semi-dark-red\" , \"value\" : 90 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 21 , \"y\" : 31 }, \"id\" : 23 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"((sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"}) - sum(node_filesystem_avail_bytes{device=~\\\".+$drive_name.?\\\"})) / sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"})) * 100\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Disk Usage\" , \"type\" : \"gauge\" } ], \"repeat\" : \"drive_name\" , \"title\" : \"Drive Stats for /dev/$drive_name\" , \"type\" : \"row\" } ], \"refresh\" : \"10s\" , \"schemaVersion\" : 36 , \"style\" : \"dark\" , \"tags\" : [ \"linux\" , \"node-exporter\" ], \"templating\" : { \"list\" : [ { \"current\" : { \"selected\" : false , \"text\" : \"node\" , \"value\" : \"node\" }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_uname_info, job)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : \"Job\" , \"multi\" : false , \"name\" : \"job\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_uname_info, job)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"type\" : \"query\" }, { \"current\" : { \"selected\" : false , \"text\" : \"host.docker.internal:9100\" , \"value\" : \"host.docker.internal:9100\" }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_uname_info{job=\\\"$job\\\"}, instance)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : \"Host\" , \"multi\" : false , \"name\" : \"node\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_uname_info{job=\\\"$job\\\"}, instance)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"type\" : \"query\" }, { \"current\" : { \"selected\" : false , \"text\" : \"enp2s0\" , \"value\" : \"enp2s0\" }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_network_info{operstate=\\\"up\\\"}, device)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : \"NIC\" , \"multi\" : false , \"name\" : \"network_interface\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_network_info{operstate=\\\"up\\\"}, device)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 0 , \"type\" : \"query\" }, { \"current\" : { \"selected\" : true , \"text\" : [ \"All\" ], \"value\" : [ \"$__all\" ] }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_disk_info, device)\" , \"description\" : \"Drive /dev/sdX\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"Drive\" , \"multi\" : true , \"name\" : \"drive_name\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_disk_info, device)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"type\" : \"query\" } ] }, \"time\" : { \"from\" : \"now-5m\" , \"to\" : \"now\" }, \"timepicker\" : { \"refresh_intervals\" : [ \"10s\" , \"1m\" , \"5m\" , \"15m\" , \"30m\" ] }, \"timezone\" : \"browser\" , \"title\" : \"Server Dashboard\" , \"uid\" : \"iUjT2qZgz\" , \"version\" : 9 , \"weekStart\" : \"\" } This dashboard will look similar to this:","title":"Grafana"},{"location":"linux/services/monitoring/grafana/#grafana","text":"Warning You should set up Prometheus first before setting this up. Grafana is a dashboard that can consume from Prometheus and other data sources to display all the information that you need. This service has an official image on Docker Hub which we'll use.","title":"Grafana"},{"location":"linux/services/monitoring/grafana/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/grafana Inside this folder, create a data folder: mkdir data","title":"Pre-Installation"},{"location":"linux/services/monitoring/grafana/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : grafana : image : grafana/grafana:latest restart : unless-stopped user : 1000:1000 ports : - 33000:3000 volumes : - ./data:/var/lib/grafana environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/monitoring/grafana/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 33000 /tcp","title":"Post-Installation"},{"location":"linux/services/monitoring/grafana/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/monitoring/grafana/#add-prometheus-to-grafana","text":"Visit the Grafana's site by accessing: http://localhost:3000 And create your Admin account. Make your way to the following page: Configuration > Data Sources > Add a data source > Prometheus And add the local IP of your server.","title":"Add Prometheus to Grafana"},{"location":"linux/services/monitoring/grafana/#add-a-dashboard","text":"Make your way to the following page. Create > Import And paste the following JSON in it: { \"annotations\" : { \"list\" : [ { \"builtIn\" : 1 , \"datasource\" : { \"type\" : \"grafana\" , \"uid\" : \"-- Grafana --\" }, \"enable\" : true , \"hide\" : true , \"iconColor\" : \"rgba(0, 211, 255, 1)\" , \"name\" : \"Annotations & Alerts\" , \"target\" : { \"limit\" : 100 , \"matchAny\" : false , \"tags\" : [], \"type\" : \"dashboard\" }, \"type\" : \"dashboard\" } ] }, \"description\" : \"moonstar's Server Dashboard\" , \"editable\" : true , \"fiscalYearStartMonth\" : 0 , \"graphTooltip\" : 0 , \"id\" : 4 , \"iteration\" : 1653438285360 , \"links\" : [], \"liveNow\" : false , \"panels\" : [ { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 0 }, \"id\" : 8 , \"panels\" : [], \"title\" : \"Quick Info\" , \"type\" : \"row\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 75 }, { \"color\" : \"semi-dark-red\" , \"value\" : 95 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 0 , \"y\" : 1 }, \"id\" : 2 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"(((count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu))) - avg(sum by (mode)(irate(node_cpu_seconds_total{mode='idle',instance=\\\"$node\\\",job=\\\"$job\\\"}[5m])))) * 100) / count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu))\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Usage\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU temperature\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"blue\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 45 }, { \"color\" : \"semi-dark-red\" , \"value\" : 65 } ] }, \"unit\" : \"celsius\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 3 , \"y\" : 1 }, \"id\" : 5 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"mean\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(node_hwmon_temp_celsius{instance=\\\"$node\\\",job=\\\"$job\\\"})\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Temp\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Sys Load (5m avg)\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 75 }, { \"color\" : \"semi-dark-red\" , \"value\" : 95 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 6 , \"y\" : 1 }, \"id\" : 6 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(node_load5{instance=\\\"$node\\\",job=\\\"$job\\\"}) / count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu)) * 100\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Sys Load\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"RAM Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 70 }, { \"color\" : \"semi-dark-red\" , \"value\" : 90 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 9 , \"y\" : 1 }, \"id\" : 3 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"100 - ((node_memory_MemAvailable_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"} * 100) / node_memory_MemTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"})\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"RAM Usage\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Swap Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 15 }, { \"color\" : \"semi-dark-red\" , \"value\" : 30 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 12 , \"y\" : 1 }, \"id\" : 4 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"((node_memory_SwapTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"} - node_memory_SwapFree_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"}) / (node_memory_SwapTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"} )) * 100\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Swap Usage\" , \"type\" : \"gauge\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Cores\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"short\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 15 , \"y\" : 1 }, \"id\" : 10 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"count(count(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\"}) by (cpu))\" , \"refId\" : \"A\" } ], \"title\" : \"CPU Cores\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"RAM Total\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 18 , \"y\" : 1 }, \"id\" : 11 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_memory_MemTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"RAM Total\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Swap Total\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 21 , \"y\" : 1 }, \"id\" : 12 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_memory_SwapTotal_bytes{instance=\\\"$node\\\",job=\\\"$job\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"Swap Total\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Uptime\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"dtdhms\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 15 , \"y\" : 3 }, \"id\" : 13 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_time_seconds{instance=\\\"$node\\\",job=\\\"$job\\\"} - node_boot_time_seconds{instance=\\\"$node\\\",job=\\\"$job\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"Uptime\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"/ Total Size\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 18 , \"y\" : 3 }, \"id\" : 14 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"node_filesystem_size_bytes{instance=\\\"$node\\\",job=\\\"$job\\\",mountpoint=\\\"/\\\",fstype!=\\\"rootfs\\\"}\" , \"refId\" : \"A\" } ], \"title\" : \"/ Total Size\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"/ Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null }, { \"color\" : \"#EAB839\" , \"value\" : 65 }, { \"color\" : \"semi-dark-red\" , \"value\" : 90 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 2 , \"w\" : 3 , \"x\" : 21 , \"y\" : 3 }, \"id\" : 15 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"expr\" : \"100 - ((node_filesystem_avail_bytes{instance=\\\"$node\\\",job=\\\"$job\\\",mountpoint=\\\"/\\\",fstype!=\\\"rootfs\\\"} * 100) / node_filesystem_size_bytes{instance=\\\"$node\\\",job=\\\"$job\\\",mountpoint=\\\"/\\\",fstype!=\\\"rootfs\\\"})\" , \"refId\" : \"A\" } ], \"title\" : \"/ Usage\" , \"type\" : \"stat\" }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 5 }, \"id\" : 26 , \"panels\" : [], \"title\" : \"Network Stats\" , \"type\" : \"row\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Bytes Received/Transferred [$dev]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 8 , \"x\" : 0 , \"y\" : 6 }, \"id\" : 28 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"node_network_receive_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}\" , \"legendFormat\" : \"Bytes Received\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"node_network_transmit_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}\" , \"hide\" : false , \"legendFormat\" : \"Bytes Transferred\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Bytes Received/Transferred\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Transfer Speed [$network_interface]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"Bps\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 8 , \"x\" : 8 , \"y\" : 6 }, \"id\" : 29 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"rate(node_network_receive_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}[5m])\" , \"instant\" : false , \"legendFormat\" : \"Receive Speed\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"rate(node_network_transmit_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$network_interface\\\"}[5m])\" , \"hide\" : false , \"legendFormat\" : \"Transmit Speed\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Transfer Speed\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"TCP Connections [$network_interface]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"short\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 8 , \"x\" : 16 , \"y\" : 6 }, \"id\" : 30 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"node_netstat_Tcp_CurrEstab\" , \"instant\" : false , \"legendFormat\" : \"Number of Connections\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"TCP Connections\" , \"type\" : \"timeseries\" }, { \"collapsed\" : false , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 14 }, \"id\" : 32 , \"panels\" : [], \"title\" : \"CPU Stats\" , \"type\" : \"row\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Core Frequency\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"rothz\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 11 , \"w\" : 8 , \"x\" : 0 , \"y\" : 15 }, \"id\" : 34 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"node_cpu_scaling_frequency_hertz\" , \"legendFormat\" : \"Core {{cpu}}\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Core Frequency\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Usage by Mode\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 11 , \"w\" : 8 , \"x\" : 8 , \"y\" : 15 }, \"id\" : 35 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" , \"sortBy\" : \"Max\" , \"sortDesc\" : true }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"system\\\"}[5m]) * 100) \" , \"instant\" : false , \"legendFormat\" : \"system\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"user\\\"}[5m]) * 100) \" , \"hide\" : false , \"legendFormat\" : \"user\" , \"range\" : true , \"refId\" : \"B\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"iowait\\\"}[5m]) * 100) \" , \"hide\" : false , \"legendFormat\" : \"iowait\" , \"range\" : true , \"refId\" : \"C\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"steal\\\"}[5m]) * 100) \" , \"hide\" : false , \"legendFormat\" : \"steal\" , \"range\" : true , \"refId\" : \"D\" } ], \"title\" : \"CPU Usage by Mode\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"CPU Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : false , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" , \"value\" : null } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 11 , \"w\" : 8 , \"x\" : 16 , \"y\" : 15 }, \"id\" : 36 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" , \"sortBy\" : \"Max\" , \"sortDesc\" : true }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"system\\\"}[5m]) * 100) + avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"user\\\"}[5m]) * 100) + avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"iowait\\\"}[5m]) * 100) + avg(rate(node_cpu_seconds_total{instance=\\\"$node\\\",job=\\\"$job\\\",mode=\\\"steal\\\"}[5m]) * 100) \" , \"instant\" : false , \"legendFormat\" : \"CPU Usage\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"CPU Usage\" , \"type\" : \"timeseries\" }, { \"collapsed\" : true , \"gridPos\" : { \"h\" : 1 , \"w\" : 24 , \"x\" : 0 , \"y\" : 26 }, \"id\" : 17 , \"panels\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Bytes Read/Written [$drive_name ]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : true , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 9 , \"x\" : 0 , \"y\" : 27 }, \"id\" : 19 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"rate(node_disk_read_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[5m])\" , \"instant\" : false , \"legendFormat\" : \"Bytes Read\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"rate(node_disk_written_bytes_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[5m])\" , \"hide\" : false , \"legendFormat\" : \"Bytes Written\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Bytes Read/Written\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"Disk IOps Completed [$drive_name ]\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"palette-classic\" }, \"custom\" : { \"axisLabel\" : \"\" , \"axisPlacement\" : \"auto\" , \"barAlignment\" : 0 , \"drawStyle\" : \"line\" , \"fillOpacity\" : 10 , \"gradientMode\" : \"none\" , \"hideFrom\" : { \"legend\" : false , \"tooltip\" : false , \"viz\" : false }, \"lineInterpolation\" : \"smooth\" , \"lineStyle\" : { \"fill\" : \"solid\" }, \"lineWidth\" : 1 , \"pointSize\" : 5 , \"scaleDistribution\" : { \"type\" : \"linear\" }, \"showPoints\" : \"never\" , \"spanNulls\" : true , \"stacking\" : { \"group\" : \"A\" , \"mode\" : \"none\" }, \"thresholdsStyle\" : { \"mode\" : \"off\" } }, \"decimals\" : 0 , \"mappings\" : [], \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" } ] }, \"unit\" : \"iops\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 8 , \"w\" : 9 , \"x\" : 9 , \"y\" : 27 }, \"id\" : 24 , \"options\" : { \"legend\" : { \"calcs\" : [ \"min\" , \"max\" , \"mean\" , \"lastNotNull\" ], \"displayMode\" : \"table\" , \"placement\" : \"bottom\" }, \"tooltip\" : { \"mode\" : \"multi\" , \"sort\" : \"none\" } }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"exemplar\" : false , \"expr\" : \"irate(node_disk_reads_completed_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[1m])\" , \"instant\" : false , \"legendFormat\" : \"Reads Completed\" , \"range\" : true , \"refId\" : \"A\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"irate(node_disk_writes_completed_total{instance=\\\"$node\\\",job=\\\"$job\\\",device=\\\"$drive_name\\\"}[1m])\" , \"hide\" : false , \"legendFormat\" : \"Writes Completed\" , \"range\" : true , \"refId\" : \"B\" } ], \"title\" : \"Disk IOps Completed\" , \"type\" : \"timeseries\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Total Size\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 18 , \"y\" : 27 }, \"id\" : 20 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"})\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Total Size\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Used Space\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"yellow\" } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 21 , \"y\" : 27 }, \"id\" : 22 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"}) - sum(node_filesystem_avail_bytes{device=~\\\".+$drive_name.?\\\"})\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Used Space\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Free Space\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"semi-dark-red\" }, { \"color\" : \"#EAB839\" , \"value\" : 20 }, { \"color\" : \"green\" , \"value\" : 50 } ] }, \"unit\" : \"decbytes\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 18 , \"y\" : 31 }, \"id\" : 21 , \"options\" : { \"colorMode\" : \"value\" , \"graphMode\" : \"none\" , \"justifyMode\" : \"auto\" , \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"textMode\" : \"auto\" }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"sum(node_filesystem_avail_bytes{device=~\\\".+$drive_name.?\\\"})\" , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Free Space\" , \"type\" : \"stat\" }, { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"description\" : \"$drive_name Disk Usage\" , \"fieldConfig\" : { \"defaults\" : { \"color\" : { \"mode\" : \"thresholds\" }, \"decimals\" : 0 , \"mappings\" : [], \"max\" : 100 , \"min\" : 0 , \"noValue\" : \"N/A\" , \"thresholds\" : { \"mode\" : \"absolute\" , \"steps\" : [ { \"color\" : \"green\" }, { \"color\" : \"#EAB839\" , \"value\" : 75 }, { \"color\" : \"semi-dark-red\" , \"value\" : 90 } ] }, \"unit\" : \"percent\" }, \"overrides\" : [] }, \"gridPos\" : { \"h\" : 4 , \"w\" : 3 , \"x\" : 21 , \"y\" : 31 }, \"id\" : 23 , \"options\" : { \"orientation\" : \"horizontal\" , \"reduceOptions\" : { \"calcs\" : [ \"lastNotNull\" ], \"fields\" : \"\" , \"values\" : false }, \"showThresholdLabels\" : false , \"showThresholdMarkers\" : true , \"text\" : {} }, \"pluginVersion\" : \"8.5.3\" , \"targets\" : [ { \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"editorMode\" : \"code\" , \"expr\" : \"((sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"}) - sum(node_filesystem_avail_bytes{device=~\\\".+$drive_name.?\\\"})) / sum(node_filesystem_size_bytes{device=~\\\".+$drive_name.?\\\"})) * 100\" , \"hide\" : false , \"range\" : true , \"refId\" : \"A\" } ], \"title\" : \"Disk Usage\" , \"type\" : \"gauge\" } ], \"repeat\" : \"drive_name\" , \"title\" : \"Drive Stats for /dev/$drive_name\" , \"type\" : \"row\" } ], \"refresh\" : \"10s\" , \"schemaVersion\" : 36 , \"style\" : \"dark\" , \"tags\" : [ \"linux\" , \"node-exporter\" ], \"templating\" : { \"list\" : [ { \"current\" : { \"selected\" : false , \"text\" : \"node\" , \"value\" : \"node\" }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_uname_info, job)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : \"Job\" , \"multi\" : false , \"name\" : \"job\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_uname_info, job)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"type\" : \"query\" }, { \"current\" : { \"selected\" : false , \"text\" : \"host.docker.internal:9100\" , \"value\" : \"host.docker.internal:9100\" }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_uname_info{job=\\\"$job\\\"}, instance)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : \"Host\" , \"multi\" : false , \"name\" : \"node\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_uname_info{job=\\\"$job\\\"}, instance)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"type\" : \"query\" }, { \"current\" : { \"selected\" : false , \"text\" : \"enp2s0\" , \"value\" : \"enp2s0\" }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_network_info{operstate=\\\"up\\\"}, device)\" , \"hide\" : 0 , \"includeAll\" : false , \"label\" : \"NIC\" , \"multi\" : false , \"name\" : \"network_interface\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_network_info{operstate=\\\"up\\\"}, device)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 0 , \"type\" : \"query\" }, { \"current\" : { \"selected\" : true , \"text\" : [ \"All\" ], \"value\" : [ \"$__all\" ] }, \"datasource\" : { \"type\" : \"prometheus\" , \"uid\" : \"UCHnO3Wgz\" }, \"definition\" : \"label_values(node_disk_info, device)\" , \"description\" : \"Drive /dev/sdX\" , \"hide\" : 0 , \"includeAll\" : true , \"label\" : \"Drive\" , \"multi\" : true , \"name\" : \"drive_name\" , \"options\" : [], \"query\" : { \"query\" : \"label_values(node_disk_info, device)\" , \"refId\" : \"StandardVariableQuery\" }, \"refresh\" : 1 , \"regex\" : \"\" , \"skipUrlSync\" : false , \"sort\" : 1 , \"type\" : \"query\" } ] }, \"time\" : { \"from\" : \"now-5m\" , \"to\" : \"now\" }, \"timepicker\" : { \"refresh_intervals\" : [ \"10s\" , \"1m\" , \"5m\" , \"15m\" , \"30m\" ] }, \"timezone\" : \"browser\" , \"title\" : \"Server Dashboard\" , \"uid\" : \"iUjT2qZgz\" , \"version\" : 9 , \"weekStart\" : \"\" } This dashboard will look similar to this:","title":"Add a Dashboard"},{"location":"linux/services/monitoring/librespeed/","text":"LibreSpeed \u00b6 LibreSpeed is a self-hosted speed test, useful to check our connection from outside into our server. There is no official docker image for LibreSpeed , however we'll use one from LinuxServer on Docker Hub . Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/librespeed Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : librespeed : image : ghcr.io/linuxserver/librespeed:latest restart : unless-stopped volumes : - ./config:/config ports : - 8050:80 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil - PASSWORD=CHANGE_THIS Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note Make sure to change CHANGE_THIS to a custom value. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8050 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"LibreSpeed"},{"location":"linux/services/monitoring/librespeed/#librespeed","text":"LibreSpeed is a self-hosted speed test, useful to check our connection from outside into our server. There is no official docker image for LibreSpeed , however we'll use one from LinuxServer on Docker Hub .","title":"LibreSpeed"},{"location":"linux/services/monitoring/librespeed/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/librespeed","title":"Pre-Installation"},{"location":"linux/services/monitoring/librespeed/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : librespeed : image : ghcr.io/linuxserver/librespeed:latest restart : unless-stopped volumes : - ./config:/config ports : - 8050:80 environment : - PUID=1000 - PGID=1000 - TZ=America/Guayaquil - PASSWORD=CHANGE_THIS Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note Make sure to change CHANGE_THIS to a custom value.","title":"Docker Compose"},{"location":"linux/services/monitoring/librespeed/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8050 /tcp","title":"Post-Installation"},{"location":"linux/services/monitoring/librespeed/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/monitoring/prometheus/","text":"Prometheus \u00b6 Prometheus is a time series database for monitoring systems, this will serve as the backend of the monitoring service. This service has an official image on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/prometheus Inside this folder, create a data folder: mkdir data And also, create a prometheus.yml file: nano prometheus.yml Which should have the following: global : scrape_interval : 15s scrape_configs : - job_name : \"prometheus\" scrape_interval : 15s static_configs : - targets : [ \"localhost:9090\" ] - job_name : \"node\" static_configs : - targets : [ \"host.docker.internal:9100\" ] Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : exporter : image : prom/node-exporter:latest restart : unless-stopped user : '1000:1000' network_mode : host volumes : - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command : - '--path.procfs=/host/proc' - '--path.rootfs=/rootfs' - '--path.sysfs=/host/sys' - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)' prometheus : image : prom/prometheus:latest restart : unless-stopped user : '1000:1000' extra_hosts : - 'host.docker.internal:host-gateway' ports : - 9090:9090 volumes : - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./data:/prometheus command : - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/etc/prometheus/console_libraries' - '--web.console.templates=/etc/prometheus/consoles' - '--web.enable-lifecycle' Note The exporter service needs to have network_mode: host in order to have access to the main network interface to actually have valid network usage data. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 9090 /tcp sudo ufw allow 9100 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Prometheus"},{"location":"linux/services/monitoring/prometheus/#prometheus","text":"Prometheus is a time series database for monitoring systems, this will serve as the backend of the monitoring service. This service has an official image on Docker Hub which we'll use.","title":"Prometheus"},{"location":"linux/services/monitoring/prometheus/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/prometheus Inside this folder, create a data folder: mkdir data And also, create a prometheus.yml file: nano prometheus.yml Which should have the following: global : scrape_interval : 15s scrape_configs : - job_name : \"prometheus\" scrape_interval : 15s static_configs : - targets : [ \"localhost:9090\" ] - job_name : \"node\" static_configs : - targets : [ \"host.docker.internal:9100\" ]","title":"Pre-Installation"},{"location":"linux/services/monitoring/prometheus/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : exporter : image : prom/node-exporter:latest restart : unless-stopped user : '1000:1000' network_mode : host volumes : - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command : - '--path.procfs=/host/proc' - '--path.rootfs=/rootfs' - '--path.sysfs=/host/sys' - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)' prometheus : image : prom/prometheus:latest restart : unless-stopped user : '1000:1000' extra_hosts : - 'host.docker.internal:host-gateway' ports : - 9090:9090 volumes : - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./data:/prometheus command : - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/etc/prometheus/console_libraries' - '--web.console.templates=/etc/prometheus/consoles' - '--web.enable-lifecycle' Note The exporter service needs to have network_mode: host in order to have access to the main network interface to actually have valid network usage data.","title":"Docker Compose"},{"location":"linux/services/monitoring/prometheus/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 9090 /tcp sudo ufw allow 9100 /tcp","title":"Post-Installation"},{"location":"linux/services/monitoring/prometheus/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/monitoring/scrutiny/","text":"Scrutiny \u00b6 Scrutiny is a S.M.A.R.T monitoring tool that uses smartd . There is no official docker image for Scrutiny , however we'll use one from LinuxServer on Docker Hub . Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/scrutiny Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : scrutiny : image : ghcr.io/linuxserver/scrutiny:8e34ef8d-ls35 restart : unless-stopped cap_add : - SYS_RAWIO volumes : - ./config:/config - /run/udev:/run/udev:ro ports : - 8020:8080 devices : - /dev/sda:/dev/sda - /dev/sdb:/dev/sdb - /dev/sdc:/dev/sdc - /dev/sdd:/dev/sdd - /dev/sde:/dev/sde environment : - PUID=1000 - PGID=1000 - SCRUTINY_WEB=true - SCRUTINY_COLLECTOR=true - TZ=America/Guayaquil Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note Under devices make sure you're passing your hard drives. You can check blkid to see which device blocks to pass. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 8020 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure. Updating Results \u00b6 To manually update the results, run the following from inside the container's data folder: docker-compose run --rm scrutiny scrutiny-collector-metrics run","title":"Scrutiny"},{"location":"linux/services/monitoring/scrutiny/#scrutiny","text":"Scrutiny is a S.M.A.R.T monitoring tool that uses smartd . There is no official docker image for Scrutiny , however we'll use one from LinuxServer on Docker Hub .","title":"Scrutiny"},{"location":"linux/services/monitoring/scrutiny/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/monitoring/scrutiny","title":"Pre-Installation"},{"location":"linux/services/monitoring/scrutiny/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : scrutiny : image : ghcr.io/linuxserver/scrutiny:8e34ef8d-ls35 restart : unless-stopped cap_add : - SYS_RAWIO volumes : - ./config:/config - /run/udev:/run/udev:ro ports : - 8020:8080 devices : - /dev/sda:/dev/sda - /dev/sdb:/dev/sdb - /dev/sdc:/dev/sdc - /dev/sdd:/dev/sdd - /dev/sde:/dev/sde environment : - PUID=1000 - PGID=1000 - SCRUTINY_WEB=true - SCRUTINY_COLLECTOR=true - TZ=America/Guayaquil Note In the case of the PUID and PGID environment variables, 1000 corresponds to the user's UID and GID respectively. You can find the values for your own user by running id $whoami . Note Under devices make sure you're passing your hard drives. You can check blkid to see which device blocks to pass.","title":"Docker Compose"},{"location":"linux/services/monitoring/scrutiny/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 8020 /tcp","title":"Post-Installation"},{"location":"linux/services/monitoring/scrutiny/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/monitoring/scrutiny/#updating-results","text":"To manually update the results, run the following from inside the container's data folder: docker-compose run --rm scrutiny scrutiny-collector-metrics run","title":"Updating Results"},{"location":"linux/services/other/","text":"Initialization \u00b6 All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to other services. mkdir ~/other For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/other/#initialization","text":"All the services inside this section will be implemented on Docker . Since we'll use Docker Compose to execute the services, we'll create a folder on the main user's home folder dedicated to other services. mkdir ~/other For each service created, there will be a subfolder where a docker-compose.yml file will be located, alongside any data volumes required and even a Dockerfile if required.","title":"Initialization"},{"location":"linux/services/other/homepage/","text":"Homepage \u00b6 Homepage is a homepage for your server, it allows to have links to your self hosted services. It also has some nice integrations with some of these services so it can display information about them from this homepage. This service has an image on GitHub Packages which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/other/homepage Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : homepage : image : ghcr.io/gethomepage/homepage:latest restart : unless-stopped ports : - 80:3000 volumes : - ./config:/app/config - /var/run/docker.sock:/var/run/docker.sock:ro - /media/sata_2tb:/media/sata_2tb:ro - /media/usb_4tb:/media/usb_4tb:ro - /media/usb_4tb_2:/media/usb_4tb_2:ro - /media/usb_8tb:/media/usb_8tb:ro environment : - TZ=America/Guayaquil Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 80 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Homepage"},{"location":"linux/services/other/homepage/#homepage","text":"Homepage is a homepage for your server, it allows to have links to your self hosted services. It also has some nice integrations with some of these services so it can display information about them from this homepage. This service has an image on GitHub Packages which we'll use.","title":"Homepage"},{"location":"linux/services/other/homepage/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/other/homepage","title":"Pre-Installation"},{"location":"linux/services/other/homepage/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : homepage : image : ghcr.io/gethomepage/homepage:latest restart : unless-stopped ports : - 80:3000 volumes : - ./config:/app/config - /var/run/docker.sock:/var/run/docker.sock:ro - /media/sata_2tb:/media/sata_2tb:ro - /media/usb_4tb:/media/usb_4tb:ro - /media/usb_4tb_2:/media/usb_4tb_2:ro - /media/usb_8tb:/media/usb_8tb:ro environment : - TZ=America/Guayaquil","title":"Docker Compose"},{"location":"linux/services/other/homepage/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 80 /tcp","title":"Post-Installation"},{"location":"linux/services/other/homepage/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/other/rtmp-simulcast/","text":"RTMP Simulcast \u00b6 If you like to stream your video games to multiple platforms simultaneously, then RTMP Simulcast is for you. There are services out there that let you do this for free, but they have some cons: Depending on the service you use, your stream titles or descriptions could be used for advertising purposes, which basically means you lose complete control of what your stream description displays to your viewers. Since these services are used by multiple people at the same time, streaming platforms will most likely detect a huge pool of users streaming from the same IP which means that you'll be targeted by a low exposure algorithm that will, ironically, make your stream harder to find. You can bypass these problems by streaming simultaneously to your desired platforms yourself, but keep in mind, there are some difficulties as well: Doing this (ideally) requires you to have a second computer (performance doesn't matter too much, you could even use a Raspberry Pi for this), you could technically achieve the same result with a virtual machine inside the streaming computer but it would represent a huge performance drop. Since you would be streaming to multiple platforms yourself, you would need a much higher upload bandwidth (around the stream bitrate multiplied by the number of platforms you're streaming to). For instance, my stream gets rendered at a bitrate of 6000 kbps , if I wanted to stream to 3 different platforms at the same time, I would need around 20000 kbps . Pre-Installation \u00b6 We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/other/rtmp-simulcast Configuration \u00b6 To configure this service, we'll first create a folder called config inside ~/other/rtmp-simulcast and create a file named rtmp.conf where the configuration for the simulcast will be located. mkdir ~/other/rtmp-simulcast/config nano ~/other/rtmp-simulcast/config/rtmp.conf And its content should be as follows: # RTMP Stream Simulcast rtmp { server { listen 1935; application live { live on; record off; push rtmp://ingest.server.com/application/stream_key } application local { live on; record off; } } } Note Replace rtmp://ingest.sever.com/application/stream_key with the actual ingest server of the platform you want to stream to. For example: rtmp://a.rtmp.youtube.com/live2/{my_stream_key} . You can push multiple ingest servers. Dockerfile \u00b6 Since the service does not have an official Docker image, we'll create a Dockerfile . The content of the Dockerfile file is as follows: FROM ubuntu:20.04 RUN apt-get update && apt-get install -y nginx libnginx-mod-rtmp WORKDIR /etc/nginx VOLUME /etc/nginx/rtmp-config RUN printf \"\\n\\nrtmp {\\n include /etc/nginx/rtmp-config/*.conf;\\n}\\n\" >> nginx.conf EXPOSE 1935 EXPOSE 80 CMD [ \"nginx\" , \"-g\" , \"daemon off;\" ] Docker Compose \u00b6 The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : rtmp : build : . restart : unless-stopped volumes : - ./config:/etc/nginx/rtmp-config ports : - 1935:1935 environment : - TZ=America/Guayaquil Before starting, we need to build this image, do so with: docker-compose build Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 1935 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure. Streaming Settings \u00b6 In your streaming software, change your streaming server to the following: rtmp://server_local_ip/live And you can place whatever you want as the stream key. For testing purposes, you can also stream to rtmp://server_local_ip/local and watch it to see how it performs. Watching Your Stream Locally \u00b6 If you wish to see how your server perceives your stream, you can use a RTMP player (VLC will do the trick), and open the stream at the url rtmp://server_local_ip/local/{stream_key} . Replace {stream_key} with the actual streaming key that you're currently using.","title":"RTMP Simulcast"},{"location":"linux/services/other/rtmp-simulcast/#rtmp-simulcast","text":"If you like to stream your video games to multiple platforms simultaneously, then RTMP Simulcast is for you. There are services out there that let you do this for free, but they have some cons: Depending on the service you use, your stream titles or descriptions could be used for advertising purposes, which basically means you lose complete control of what your stream description displays to your viewers. Since these services are used by multiple people at the same time, streaming platforms will most likely detect a huge pool of users streaming from the same IP which means that you'll be targeted by a low exposure algorithm that will, ironically, make your stream harder to find. You can bypass these problems by streaming simultaneously to your desired platforms yourself, but keep in mind, there are some difficulties as well: Doing this (ideally) requires you to have a second computer (performance doesn't matter too much, you could even use a Raspberry Pi for this), you could technically achieve the same result with a virtual machine inside the streaming computer but it would represent a huge performance drop. Since you would be streaming to multiple platforms yourself, you would need a much higher upload bandwidth (around the stream bitrate multiplied by the number of platforms you're streaming to). For instance, my stream gets rendered at a bitrate of 6000 kbps , if I wanted to stream to 3 different platforms at the same time, I would need around 20000 kbps .","title":"RTMP Simulcast"},{"location":"linux/services/other/rtmp-simulcast/#pre-installation","text":"We'll create a folder in the main user's home where all the service's data will be saved. mkdir ~/other/rtmp-simulcast","title":"Pre-Installation"},{"location":"linux/services/other/rtmp-simulcast/#configuration","text":"To configure this service, we'll first create a folder called config inside ~/other/rtmp-simulcast and create a file named rtmp.conf where the configuration for the simulcast will be located. mkdir ~/other/rtmp-simulcast/config nano ~/other/rtmp-simulcast/config/rtmp.conf And its content should be as follows: # RTMP Stream Simulcast rtmp { server { listen 1935; application live { live on; record off; push rtmp://ingest.server.com/application/stream_key } application local { live on; record off; } } } Note Replace rtmp://ingest.sever.com/application/stream_key with the actual ingest server of the platform you want to stream to. For example: rtmp://a.rtmp.youtube.com/live2/{my_stream_key} . You can push multiple ingest servers.","title":"Configuration"},{"location":"linux/services/other/rtmp-simulcast/#dockerfile","text":"Since the service does not have an official Docker image, we'll create a Dockerfile . The content of the Dockerfile file is as follows: FROM ubuntu:20.04 RUN apt-get update && apt-get install -y nginx libnginx-mod-rtmp WORKDIR /etc/nginx VOLUME /etc/nginx/rtmp-config RUN printf \"\\n\\nrtmp {\\n include /etc/nginx/rtmp-config/*.conf;\\n}\\n\" >> nginx.conf EXPOSE 1935 EXPOSE 80 CMD [ \"nginx\" , \"-g\" , \"daemon off;\" ]","title":"Dockerfile"},{"location":"linux/services/other/rtmp-simulcast/#docker-compose","text":"The service will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : rtmp : build : . restart : unless-stopped volumes : - ./config:/etc/nginx/rtmp-config ports : - 1935:1935 environment : - TZ=America/Guayaquil Before starting, we need to build this image, do so with: docker-compose build","title":"Docker Compose"},{"location":"linux/services/other/rtmp-simulcast/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 1935 /tcp","title":"Post-Installation"},{"location":"linux/services/other/rtmp-simulcast/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"linux/services/other/rtmp-simulcast/#streaming-settings","text":"In your streaming software, change your streaming server to the following: rtmp://server_local_ip/live And you can place whatever you want as the stream key. For testing purposes, you can also stream to rtmp://server_local_ip/local and watch it to see how it performs.","title":"Streaming Settings"},{"location":"linux/services/other/rtmp-simulcast/#watching-your-stream-locally","text":"If you wish to see how your server perceives your stream, you can use a RTMP player (VLC will do the trick), and open the stream at the url rtmp://server_local_ip/local/{stream_key} . Replace {stream_key} with the actual streaming key that you're currently using.","title":"Watching Your Stream Locally"},{"location":"linux/services/other/rxresume/","text":"Reactive Resume \u00b6 Reactive Resume is a tool to easily create beautiful CVs. This service has an official image available on Docker Hub which we'll use. Pre-Installation \u00b6 We'll create a folder in the main user's home where all the other server's data will be saved. mkdir ~/other/rxresume Docker Compose \u00b6 Reactive Resume will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : db : image : postgres:alpine restart : unless-stopped volumes : - ./data:/var/lib/postgresql/data environment : - TZ=America/Guayaquil - POSTGRES_DB=rxresume - POSTGRES_USER=rxresume - POSTGRES_PASSWORD=CHANGE_THIS_DB_PASSWORD server : image : amruthpillai/reactive-resume:server-latest restart : unless-stopped depends_on : - db ports : - 40100:3100 environment : - TZ=America/Guayaquil - PUBLIC_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40000 - PUBLIC_SERVER_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40100 - POSTGRES_DB=rxresume - POSTGRES_USER=rxresume - POSTGRES_PASSWORD=CHANGE_THIS_DB_PASSWORD - SECRET_KEY=CHANGE_THIS_SECRET - POSTGRES_HOST=db - POSTGRES_PORT=5432 - JWT_SECRET=CHANGE_THIS_JWT_SECRET - JWT_EXPIRY_TIME=604800 client : image : amruthpillai/reactive-resume:client-latest restart : unless-stopped depends_on : - server ports : - 40000:3000 environment : - TZ=America/Guayaquil - PUBLIC_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40000 - PUBLIC_SERVER_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40100 Note Make sure to change all the environment variables that begin with CHANGE_THIS in your own configuration. Post-Installation \u00b6 We'll need to allow the service's port on our firewall. sudo ufw allow 40000 /tcp sudo ufw allow 40100 /tcp Running \u00b6 Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Reactive Resume"},{"location":"linux/services/other/rxresume/#reactive-resume","text":"Reactive Resume is a tool to easily create beautiful CVs. This service has an official image available on Docker Hub which we'll use.","title":"Reactive Resume"},{"location":"linux/services/other/rxresume/#pre-installation","text":"We'll create a folder in the main user's home where all the other server's data will be saved. mkdir ~/other/rxresume","title":"Pre-Installation"},{"location":"linux/services/other/rxresume/#docker-compose","text":"Reactive Resume will be run using Docker Compose . The content of the docker-compose.yml file is as follows: version : \"3.9\" services : db : image : postgres:alpine restart : unless-stopped volumes : - ./data:/var/lib/postgresql/data environment : - TZ=America/Guayaquil - POSTGRES_DB=rxresume - POSTGRES_USER=rxresume - POSTGRES_PASSWORD=CHANGE_THIS_DB_PASSWORD server : image : amruthpillai/reactive-resume:server-latest restart : unless-stopped depends_on : - db ports : - 40100:3100 environment : - TZ=America/Guayaquil - PUBLIC_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40000 - PUBLIC_SERVER_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40100 - POSTGRES_DB=rxresume - POSTGRES_USER=rxresume - POSTGRES_PASSWORD=CHANGE_THIS_DB_PASSWORD - SECRET_KEY=CHANGE_THIS_SECRET - POSTGRES_HOST=db - POSTGRES_PORT=5432 - JWT_SECRET=CHANGE_THIS_JWT_SECRET - JWT_EXPIRY_TIME=604800 client : image : amruthpillai/reactive-resume:client-latest restart : unless-stopped depends_on : - server ports : - 40000:3000 environment : - TZ=America/Guayaquil - PUBLIC_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40000 - PUBLIC_SERVER_URL=http://CHANGE_THIS_IP_OR_DOMAIN:40100 Note Make sure to change all the environment variables that begin with CHANGE_THIS in your own configuration.","title":"Docker Compose"},{"location":"linux/services/other/rxresume/#post-installation","text":"We'll need to allow the service's port on our firewall. sudo ufw allow 40000 /tcp sudo ufw allow 40100 /tcp","title":"Post-Installation"},{"location":"linux/services/other/rxresume/#running","text":"Start up the service with: docker-compose up -d That's it! The service will auto-start on system startup and restart on failure.","title":"Running"},{"location":"setting-up/linux/","text":"Introduction \u00b6 This section details the configuration process of the server computer running linux. Server Specs \u00b6 The server computer is a self-built desktop with the following specs: OS : Ubuntu Server 20.04 LTS 64-bit CPU : Intel Core i3-4170 (4 Threads) @3.70GHz RAM : 2x8GB DDR3 @1600MHz (16GB - Dual Channel) Motherboard : Biostar H81MHV3 Storage : 256GB Crucial BX500 SATA SSD 2TB WD Green 5400RPM SATA HDD 4TB WD Elements USB 3.0 HDD 4TB Toshiba USB 3.0 HDD 8TB Seagate USB 3.0 HDD","title":"0. Introduction"},{"location":"setting-up/linux/#introduction","text":"This section details the configuration process of the server computer running linux.","title":"Introduction"},{"location":"setting-up/linux/#server-specs","text":"The server computer is a self-built desktop with the following specs: OS : Ubuntu Server 20.04 LTS 64-bit CPU : Intel Core i3-4170 (4 Threads) @3.70GHz RAM : 2x8GB DDR3 @1600MHz (16GB - Dual Channel) Motherboard : Biostar H81MHV3 Storage : 256GB Crucial BX500 SATA SSD 2TB WD Green 5400RPM SATA HDD 4TB WD Elements USB 3.0 HDD 4TB Toshiba USB 3.0 HDD 8TB Seagate USB 3.0 HDD","title":"Server Specs"},{"location":"setting-up/linux/custom-scripts/","text":"Custom Scripts \u00b6 We will create some custom scripts that will help us with certain tasks. For this, we'll create the following folder: mkdir -p ~/.local/bin Then inside this folder we'll insert all the scripts that we'll add here. Make sure to make them executable with: chmod +x <file> docker-update \u00b6 We'll use this script to manually update docker-compose containers. Usage Run docker-update inside the folder where docker-compose.yml is located to update the container images used. #!/bin/bash echo \"Stopping containers...\" docker-compose stop echo \"Removing containers...\" docker-compose rm -f echo \"Pulling images...\" docker-compose pull echo \"Restarting containers...\" docker-compose up -d","title":"5. Custom Scripts"},{"location":"setting-up/linux/custom-scripts/#custom-scripts","text":"We will create some custom scripts that will help us with certain tasks. For this, we'll create the following folder: mkdir -p ~/.local/bin Then inside this folder we'll insert all the scripts that we'll add here. Make sure to make them executable with: chmod +x <file>","title":"Custom Scripts"},{"location":"setting-up/linux/custom-scripts/#docker-update","text":"We'll use this script to manually update docker-compose containers. Usage Run docker-update inside the folder where docker-compose.yml is located to update the container images used. #!/bin/bash echo \"Stopping containers...\" docker-compose stop echo \"Removing containers...\" docker-compose rm -f echo \"Pulling images...\" docker-compose pull echo \"Restarting containers...\" docker-compose up -d","title":"docker-update"},{"location":"setting-up/linux/docker/","text":"Docker \u00b6 A good portion of our services will be run through Docker . We'll need to first install this. Installation \u00b6 First we'll need to install some dependencies: sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release We'll then need to add the Docker repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Finally, we'll update the repositories and install Docker : sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Permissions \u00b6 We'll add the required permissions for our user into the docker group. sudo groupadd docker sudo gpasswd -a $USER docker Finally, reboot the server for the changes to apply. Docker Compose \u00b6 We'll be using Docker Compose to run our containers, to install it run the following: sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose","title":"4. Docker"},{"location":"setting-up/linux/docker/#docker","text":"A good portion of our services will be run through Docker . We'll need to first install this.","title":"Docker"},{"location":"setting-up/linux/docker/#installation","text":"First we'll need to install some dependencies: sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release We'll then need to add the Docker repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Finally, we'll update the repositories and install Docker : sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io","title":"Installation"},{"location":"setting-up/linux/docker/#permissions","text":"We'll add the required permissions for our user into the docker group. sudo groupadd docker sudo gpasswd -a $USER docker Finally, reboot the server for the changes to apply.","title":"Permissions"},{"location":"setting-up/linux/docker/#docker-compose","text":"We'll be using Docker Compose to run our containers, to install it run the following: sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose","title":"Docker Compose"},{"location":"setting-up/linux/mounting-drives/","text":"Mounting Drives \u00b6 In this server there will be a 2TB SATA drive, and 2 USB 3.0 drives of 1TB and 4TB respectively. We need these drives to mount on system startup, for this, we'll need to set up the fstab . Getting Drives' UUIDs \u00b6 In order to get the UUIDs of the drives in question, it is necessary to plug them in and reboot the server. Once this is done, execute the following: lsblk -o NAME,FSTYPE,UUID This will give an output like the following: NAME FSTYPE UUID loop0 squashfs loop1 squashfs loop2 squashfs loop3 squashfs loop4 squashfs loop5 squashfs loop6 squashfs loop7 squashfs sda \u2514\u2500sda1 ext4 e8f133c2-af07-4a7a-a42f-21f857a9a06f sdb \u251c\u2500sdb1 vfat A12E-F096 \u2514\u2500sdb2 ext4 c1aafa14-c857-4d3e-8ae1-00c8cd462810 sdc ext4 418987da-2351-11e9-aec8-b8975ad7798b sdd \u2514\u2500sdd1 ext4 43fbd170-3eff-40ec-95fc-c6cc090a5bc9 sde \u2514\u2500sde1 ext4 fedd9ed1-d4cf-4d3c-b105-7b3296f157b4 In this case, /dev/sda1 is the 1TB USB hard drive with ext4 format, /dev/sdc is the 2TB SATA drive with ext4 format, /dev/sdd1 is the 4TB USB hard drive with ext4 format, and lastly /dev/sde1 is the 8TB USB drive. We'll need the UUIDs later, so copy them somewhere. Preparing the Folders \u00b6 We'll mount the hard drives in /media , for this we'll create the required folders like so: sudo mkdir /media/sata_2tb /media/usb_1tb /media/usb_4tb /media/usb_8tb You may also change the permissions on these folders if it causes any problem. sudo chmod -R 777 /media/sata_2tb /media/usb_1tb /media/usb_4tb /media/usb_8tb Modifying fstab \u00b6 Danger Proceed at your own risk, messing up this file will most probably break your computer. You can still fix it by entering safe mode and logging in as root to rollback. We'll modify the /etc/fstab file. sudo nano /etc/fstab And we'll add a new line for each hard drive with the following structure: UUID=$UUID $DIR $FORMAT defaults 0 0 In our case, we'll add the following lines: UUID=e8f133c2-af07-4a7a-a42f-21f857a9a06f /media/usb_1tb ext4 defaults 0 0 UUID=418987da-2351-11e9-aec8-b8975ad7798b /media/sata_2tb ext4 defaults 0 0 UUID=fedd9ed1-d4cf-4d3c-b105-7b3296f157b4 /media/usb_4tb ext4 defaults 0 0 UUID=43fbd170-3eff-40ec-95fc-c6cc090a5bc9 /media/usb_8tb ext4 defaults 0 0 Finally, reboot the server. The hard drives should now be automatically mounted.","title":"3. Mounting Drives"},{"location":"setting-up/linux/mounting-drives/#mounting-drives","text":"In this server there will be a 2TB SATA drive, and 2 USB 3.0 drives of 1TB and 4TB respectively. We need these drives to mount on system startup, for this, we'll need to set up the fstab .","title":"Mounting Drives"},{"location":"setting-up/linux/mounting-drives/#getting-drives-uuids","text":"In order to get the UUIDs of the drives in question, it is necessary to plug them in and reboot the server. Once this is done, execute the following: lsblk -o NAME,FSTYPE,UUID This will give an output like the following: NAME FSTYPE UUID loop0 squashfs loop1 squashfs loop2 squashfs loop3 squashfs loop4 squashfs loop5 squashfs loop6 squashfs loop7 squashfs sda \u2514\u2500sda1 ext4 e8f133c2-af07-4a7a-a42f-21f857a9a06f sdb \u251c\u2500sdb1 vfat A12E-F096 \u2514\u2500sdb2 ext4 c1aafa14-c857-4d3e-8ae1-00c8cd462810 sdc ext4 418987da-2351-11e9-aec8-b8975ad7798b sdd \u2514\u2500sdd1 ext4 43fbd170-3eff-40ec-95fc-c6cc090a5bc9 sde \u2514\u2500sde1 ext4 fedd9ed1-d4cf-4d3c-b105-7b3296f157b4 In this case, /dev/sda1 is the 1TB USB hard drive with ext4 format, /dev/sdc is the 2TB SATA drive with ext4 format, /dev/sdd1 is the 4TB USB hard drive with ext4 format, and lastly /dev/sde1 is the 8TB USB drive. We'll need the UUIDs later, so copy them somewhere.","title":"Getting Drives' UUIDs"},{"location":"setting-up/linux/mounting-drives/#preparing-the-folders","text":"We'll mount the hard drives in /media , for this we'll create the required folders like so: sudo mkdir /media/sata_2tb /media/usb_1tb /media/usb_4tb /media/usb_8tb You may also change the permissions on these folders if it causes any problem. sudo chmod -R 777 /media/sata_2tb /media/usb_1tb /media/usb_4tb /media/usb_8tb","title":"Preparing the Folders"},{"location":"setting-up/linux/mounting-drives/#modifying-fstab","text":"Danger Proceed at your own risk, messing up this file will most probably break your computer. You can still fix it by entering safe mode and logging in as root to rollback. We'll modify the /etc/fstab file. sudo nano /etc/fstab And we'll add a new line for each hard drive with the following structure: UUID=$UUID $DIR $FORMAT defaults 0 0 In our case, we'll add the following lines: UUID=e8f133c2-af07-4a7a-a42f-21f857a9a06f /media/usb_1tb ext4 defaults 0 0 UUID=418987da-2351-11e9-aec8-b8975ad7798b /media/sata_2tb ext4 defaults 0 0 UUID=fedd9ed1-d4cf-4d3c-b105-7b3296f157b4 /media/usb_4tb ext4 defaults 0 0 UUID=43fbd170-3eff-40ec-95fc-c6cc090a5bc9 /media/usb_8tb ext4 defaults 0 0 Finally, reboot the server. The hard drives should now be automatically mounted.","title":"Modifying fstab"},{"location":"setting-up/linux/networking/","text":"Networking \u00b6 Here's a list of ports that are exposed to the host machine. This serves as a reference to prevent setting services to ports that are already in use. Port Forwarding Tables \u00b6 Native Services \u00b6 Service Port Range Protocol SSH 22 TCP Samba (SMB/CIFS) 445 TCP Analytics Services \u00b6 Service Port Range Protocol Plausible 10500 TCP Automation Services \u00b6 Service Port Range Protocol Custom Automation Service 5678 TCP Homebridge (Web) 8581 TCP Homebridge (HomeKit) 51845 TCP/UDP Docker Services \u00b6 Service Port Range Protocol Portainer 8000, 9000 TCP Fleet 8080 TCP Data Services \u00b6 Service Port Range Protocol Actual 5006 TCP Gitea 3000 TCP Jenkins 10800 TCP Nextcloud 9020 TCP Games \u00b6 Service Port Range Protocol Assetto Corsa (Manager) 8772 TCP Assetto Corsa (Server) 9600 TCP/UDP Assetto Corsa (HTTP) 8081 TCP TeamSpeak 3 (Voice) 9987 UDP TeamSpeak 3 (ServerQuery) 10011 TCP TeamSpeak 3 (FileTransfer) 30033 TCP Media Services \u00b6 Service Port Range Protocol FreshRSS 5200 TCP Jellyfin 8086 TCP EmbyStat 6555 TCP Kavita 5000 TCP Transmission 9091 TCP JDownloader 3129, 5800 TCP Ombi 3579 TCP Sonarr 8989 TCP Radarr 7878 TCP Lidarr 8686 TCP Deemix 6596 TCP Bazarr 6767 TCP Readarr 8787 TCP Prowlarr 9696 TCP OpenBooks 12450 TCP PiGallery 2 20500 TCP Plex 32400 TCP Tautulli 8181 TCP Synclounge 8088 TCP Reddit Slideshow 26969 TCP Monitoring Services \u00b6 Service Port Range Protocol Grafana 33000 TCP LibreSpeed 8050 TCP Prometheus 9090, 9100 TCP Scrutiny 8020 TCP Other Services \u00b6 Service Port Range Protocol Homepage 80 TCP Reactive Resume (Client) 40000 TCP Reactive Resume (Server) 40100 TCP RTMP Simulcast 1935 TCP Information \u00b6 If you don't know what internal IP the server is running on, you can always type on the terminal: ifconfig UFW \u00b6 UFW is a friendly frontend for iptables that makes it a lot easier to add connection rules to your firewall. One of the many wonders of UFW is the fact that rules are automatically saved when set, which is not true for iptables . Installation \u00b6 We'll need to install UFW and set it up. sudo apt-get install ufw sudo ufw default allow outgoing sudo ufw default deny incoming sudo ufw enable The commands should be pretty self-explanatory but, just in case, these default the firewall to allow any connections going from the server to the Internet and deny any incoming connections from the Internet to the server. Usage \u00b6 A very good command to check UFW 's status (if it's enabled or disabled) and see all the custom rules added and active is: sudo ufw status You can add a new rule by using: sudo ufw allow <PORT_RANGE>/<PROTOCOL> SSH \u00b6 Installation \u00b6 Since the server will run in headless mode, it would be very useful to be able to access it remotely from within (and even outside) the network. We'll use OpenSSH , it is generally installed with the OS but in case that it isn't, you can install it by running: sudo apt-get install openssh-client openssh-server Setting-up \u00b6 We now need to allow SSH connections through the firewall so we can access the server. sudo ufw allow ssh Google Authentication (2FA) \u00b6 Two Factor Authentication has become a must-have in terms of account security, almost all services out there have the option to add this security measure to ensure account security. Luckily, we can implement our own Two Factor Authentication to access the server through SSH . Installation \u00b6 To enable 2FA , we'll need to install the following package: sudo apt-get install libpam-google-authenticator Setting-up \u00b6 To set it up, simply run: google-authenticator When running this, you'll receive a secret key which is used to add your account manually to your phone's 2FA application. Alternatively, you also get a nicely printed QR code on the terminal window (which you may need to resize to see fully) that you can scan with your phone. You will also get some scratch codes that you should always keep somewhere safe, just in case you lose access to your phone or something happens, you can still login to your server. To continue, answer y to all the questions to set up 2FA with the default settings. We now need to enable 2FA on SSH , to do this, edit the following file: sudo nano /etc/ssh/sshd_config Look for the following lines and edit them accordingly: UsePAM yes ChallengeResponseAuthentication yes Save and close the editor and restart the SSH service. sudo systemctl restart ssh We now need to edit the PAM rule file: sudo nano /etc/pam.d/sshd At the end of the file, add the following line: auth required pam_google_authenticator.so Save and close the file. Testing \u00b6 In order to test that 2FA works properly, open up a new SSH session without closing the previous one and try logging in, you'll be prompted for your user password and for the 2FA code which is available on your phone. Info When using 2FA for SSH , all the users in the server will need to set-it up, otherwise they won't be able to access their accounts. In case you get locked out from one of these users, you can always login to a sudoer account (usually the admin one which is added when installing the OS) and force-login with: sudo -iu <user> .","title":"6. Networking"},{"location":"setting-up/linux/networking/#networking","text":"Here's a list of ports that are exposed to the host machine. This serves as a reference to prevent setting services to ports that are already in use.","title":"Networking"},{"location":"setting-up/linux/networking/#port-forwarding-tables","text":"","title":"Port Forwarding Tables"},{"location":"setting-up/linux/networking/#native-services","text":"Service Port Range Protocol SSH 22 TCP Samba (SMB/CIFS) 445 TCP","title":"Native Services"},{"location":"setting-up/linux/networking/#analytics-services","text":"Service Port Range Protocol Plausible 10500 TCP","title":"Analytics Services"},{"location":"setting-up/linux/networking/#automation-services","text":"Service Port Range Protocol Custom Automation Service 5678 TCP Homebridge (Web) 8581 TCP Homebridge (HomeKit) 51845 TCP/UDP","title":"Automation Services"},{"location":"setting-up/linux/networking/#docker-services","text":"Service Port Range Protocol Portainer 8000, 9000 TCP Fleet 8080 TCP","title":"Docker Services"},{"location":"setting-up/linux/networking/#data-services","text":"Service Port Range Protocol Actual 5006 TCP Gitea 3000 TCP Jenkins 10800 TCP Nextcloud 9020 TCP","title":"Data Services"},{"location":"setting-up/linux/networking/#games","text":"Service Port Range Protocol Assetto Corsa (Manager) 8772 TCP Assetto Corsa (Server) 9600 TCP/UDP Assetto Corsa (HTTP) 8081 TCP TeamSpeak 3 (Voice) 9987 UDP TeamSpeak 3 (ServerQuery) 10011 TCP TeamSpeak 3 (FileTransfer) 30033 TCP","title":"Games"},{"location":"setting-up/linux/networking/#media-services","text":"Service Port Range Protocol FreshRSS 5200 TCP Jellyfin 8086 TCP EmbyStat 6555 TCP Kavita 5000 TCP Transmission 9091 TCP JDownloader 3129, 5800 TCP Ombi 3579 TCP Sonarr 8989 TCP Radarr 7878 TCP Lidarr 8686 TCP Deemix 6596 TCP Bazarr 6767 TCP Readarr 8787 TCP Prowlarr 9696 TCP OpenBooks 12450 TCP PiGallery 2 20500 TCP Plex 32400 TCP Tautulli 8181 TCP Synclounge 8088 TCP Reddit Slideshow 26969 TCP","title":"Media Services"},{"location":"setting-up/linux/networking/#monitoring-services","text":"Service Port Range Protocol Grafana 33000 TCP LibreSpeed 8050 TCP Prometheus 9090, 9100 TCP Scrutiny 8020 TCP","title":"Monitoring Services"},{"location":"setting-up/linux/networking/#other-services","text":"Service Port Range Protocol Homepage 80 TCP Reactive Resume (Client) 40000 TCP Reactive Resume (Server) 40100 TCP RTMP Simulcast 1935 TCP","title":"Other Services"},{"location":"setting-up/linux/networking/#information","text":"If you don't know what internal IP the server is running on, you can always type on the terminal: ifconfig","title":"Information"},{"location":"setting-up/linux/networking/#ufw","text":"UFW is a friendly frontend for iptables that makes it a lot easier to add connection rules to your firewall. One of the many wonders of UFW is the fact that rules are automatically saved when set, which is not true for iptables .","title":"UFW"},{"location":"setting-up/linux/networking/#installation","text":"We'll need to install UFW and set it up. sudo apt-get install ufw sudo ufw default allow outgoing sudo ufw default deny incoming sudo ufw enable The commands should be pretty self-explanatory but, just in case, these default the firewall to allow any connections going from the server to the Internet and deny any incoming connections from the Internet to the server.","title":"Installation"},{"location":"setting-up/linux/networking/#usage","text":"A very good command to check UFW 's status (if it's enabled or disabled) and see all the custom rules added and active is: sudo ufw status You can add a new rule by using: sudo ufw allow <PORT_RANGE>/<PROTOCOL>","title":"Usage"},{"location":"setting-up/linux/networking/#ssh","text":"","title":"SSH"},{"location":"setting-up/linux/networking/#installation_1","text":"Since the server will run in headless mode, it would be very useful to be able to access it remotely from within (and even outside) the network. We'll use OpenSSH , it is generally installed with the OS but in case that it isn't, you can install it by running: sudo apt-get install openssh-client openssh-server","title":"Installation"},{"location":"setting-up/linux/networking/#setting-up","text":"We now need to allow SSH connections through the firewall so we can access the server. sudo ufw allow ssh","title":"Setting-up"},{"location":"setting-up/linux/networking/#google-authentication-2fa","text":"Two Factor Authentication has become a must-have in terms of account security, almost all services out there have the option to add this security measure to ensure account security. Luckily, we can implement our own Two Factor Authentication to access the server through SSH .","title":"Google Authentication (2FA)"},{"location":"setting-up/linux/networking/#installation_2","text":"To enable 2FA , we'll need to install the following package: sudo apt-get install libpam-google-authenticator","title":"Installation"},{"location":"setting-up/linux/networking/#setting-up_1","text":"To set it up, simply run: google-authenticator When running this, you'll receive a secret key which is used to add your account manually to your phone's 2FA application. Alternatively, you also get a nicely printed QR code on the terminal window (which you may need to resize to see fully) that you can scan with your phone. You will also get some scratch codes that you should always keep somewhere safe, just in case you lose access to your phone or something happens, you can still login to your server. To continue, answer y to all the questions to set up 2FA with the default settings. We now need to enable 2FA on SSH , to do this, edit the following file: sudo nano /etc/ssh/sshd_config Look for the following lines and edit them accordingly: UsePAM yes ChallengeResponseAuthentication yes Save and close the editor and restart the SSH service. sudo systemctl restart ssh We now need to edit the PAM rule file: sudo nano /etc/pam.d/sshd At the end of the file, add the following line: auth required pam_google_authenticator.so Save and close the file.","title":"Setting-up"},{"location":"setting-up/linux/networking/#testing","text":"In order to test that 2FA works properly, open up a new SSH session without closing the previous one and try logging in, you'll be prompted for your user password and for the 2FA code which is available on your phone. Info When using 2FA for SSH , all the users in the server will need to set-it up, otherwise they won't be able to access their accounts. In case you get locked out from one of these users, you can always login to a sudoer account (usually the admin one which is added when installing the OS) and force-login with: sudo -iu <user> .","title":"Testing"},{"location":"setting-up/linux/os-installation/","text":"OS Installation \u00b6 The server will be running Ubuntu Server 20.04 LTS 64-bit in headless mode, meaning that no DE will be used. The installation is quick and assisted by its own install wizard. When asked what snapshot (initial server configuration) should be installed, simply choose none or default . As a general rule of thumb, after installing the OS it is recommended to update the sources ad packages: sudo apt-get update && sudo apt-get upgrade Configuring Date and Time \u00b6 By default, the OS will be installed with GMT+0 as the timezone. We'll change this to conform with our real timezone which is GMT-5 . sudo timedatectl set-timezone America/Guayaquil","title":"1. OS Installation"},{"location":"setting-up/linux/os-installation/#os-installation","text":"The server will be running Ubuntu Server 20.04 LTS 64-bit in headless mode, meaning that no DE will be used. The installation is quick and assisted by its own install wizard. When asked what snapshot (initial server configuration) should be installed, simply choose none or default . As a general rule of thumb, after installing the OS it is recommended to update the sources ad packages: sudo apt-get update && sudo apt-get upgrade","title":"OS Installation"},{"location":"setting-up/linux/os-installation/#configuring-date-and-time","text":"By default, the OS will be installed with GMT+0 as the timezone. We'll change this to conform with our real timezone which is GMT-5 . sudo timedatectl set-timezone America/Guayaquil","title":"Configuring Date and Time"},{"location":"setting-up/linux/packages/","text":"Packages \u00b6 We'll install a handful of packages once we're done with the OS installation. Screen \u00b6 We'll also require screen which is a wonderful tool capable of improving multitasking on a single shell window with the ability to background processes. It also lets you recover shell windows when there's a connection loss. Installation \u00b6 Installing screen is as simple as running the following command: sudo apt-get install screen Configuration \u00b6 Create a ~/.screenrc file and add the following: termcapinfo xterm* ti@:te@ This enables the mouse wheel inside screen . Usage \u00b6 At first it may seem a little complicated and maybe even intimidating to use this tool but once you get used to it you'll realize how useful it really is. First start up screen by typing: screen -S <socket name> Here's a table with the keys and actions you can use with screen : Keys Action Ctrl-a c Creates a new window. Ctrl-a n Switches between windows. Ctrl-a d Detaches from screen. Ctrl-a n Switches between screens. To reattach to a screen : screen -r <screen pid> or, screen -rd <screen socket name> The Rest \u00b6 Install the rest of the packages with the following: sudo apt-get install net-tools neofetch nload progress","title":"2. Packages"},{"location":"setting-up/linux/packages/#packages","text":"We'll install a handful of packages once we're done with the OS installation.","title":"Packages"},{"location":"setting-up/linux/packages/#screen","text":"We'll also require screen which is a wonderful tool capable of improving multitasking on a single shell window with the ability to background processes. It also lets you recover shell windows when there's a connection loss.","title":"Screen"},{"location":"setting-up/linux/packages/#installation","text":"Installing screen is as simple as running the following command: sudo apt-get install screen","title":"Installation"},{"location":"setting-up/linux/packages/#configuration","text":"Create a ~/.screenrc file and add the following: termcapinfo xterm* ti@:te@ This enables the mouse wheel inside screen .","title":"Configuration"},{"location":"setting-up/linux/packages/#usage","text":"At first it may seem a little complicated and maybe even intimidating to use this tool but once you get used to it you'll realize how useful it really is. First start up screen by typing: screen -S <socket name> Here's a table with the keys and actions you can use with screen : Keys Action Ctrl-a c Creates a new window. Ctrl-a n Switches between windows. Ctrl-a d Detaches from screen. Ctrl-a n Switches between screens. To reattach to a screen : screen -r <screen pid> or, screen -rd <screen socket name>","title":"Usage"},{"location":"setting-up/linux/packages/#the-rest","text":"Install the rest of the packages with the following: sudo apt-get install net-tools neofetch nload progress","title":"The Rest"},{"location":"setting-up/mac-mini/","text":"Introduction \u00b6 This section details the configuration process of the Mac Mini server which is also configured with Bootcamp to dual boot Windows. Server Specs \u00b6 The Mac Mini server has the following specs: Main OS : macOS Catalina 10.15.7 Bootcamp OS : Windows 10 Home Model : Mac Mini (Late 2012) CPU : Intel Core i7-3615QM (8 Threads) @2.30GHz RAM : 2x8GB DDR3 @1600MHz (16GB - Dual Channel) Storage : 256GB Kingston SATA SSD 1TB Apple SATA HDD","title":"0. Introduction"},{"location":"setting-up/mac-mini/#introduction","text":"This section details the configuration process of the Mac Mini server which is also configured with Bootcamp to dual boot Windows.","title":"Introduction"},{"location":"setting-up/mac-mini/#server-specs","text":"The Mac Mini server has the following specs: Main OS : macOS Catalina 10.15.7 Bootcamp OS : Windows 10 Home Model : Mac Mini (Late 2012) CPU : Intel Core i7-3615QM (8 Threads) @2.30GHz RAM : 2x8GB DDR3 @1600MHz (16GB - Dual Channel) Storage : 256GB Kingston SATA SSD 1TB Apple SATA HDD","title":"Server Specs"},{"location":"setting-up/mac-mini/bootcamp/","text":"Bootcamp \u00b6 Bootcamp is a tool made by Apple to allow users to dual boot Windows on their Intel based Mac computers. The assistant comes already installed on all macOS version, so there is no need to install anything else. This page will not describe the process of installing Windows since it's mostly straightforward. You can acquire Windows 10 image from Microsoft . For our use case, we're leaving macOS with 150GB and Windows with 100GB on the main drive.","title":"2. Bootcamp"},{"location":"setting-up/mac-mini/bootcamp/#bootcamp","text":"Bootcamp is a tool made by Apple to allow users to dual boot Windows on their Intel based Mac computers. The assistant comes already installed on all macOS version, so there is no need to install anything else. This page will not describe the process of installing Windows since it's mostly straightforward. You can acquire Windows 10 image from Microsoft . For our use case, we're leaving macOS with 150GB and Windows with 100GB on the main drive.","title":"Bootcamp"},{"location":"setting-up/mac-mini/mac-os-configuration/","text":"macOS Configuration \u00b6 Note This Mac Mini in macOS will operate mainly as a Jenkins agent. Since the installation of macOS is graphically based there will not be any step by step indications of how to do it. Instead, this page specifies the software installed on this computer. Applications \u00b6 Here's a list of all the applications installed on the Mac Mini in macOS. Chrome Remote Desktop Docker Desktop Google Chrome Homebrew iTerm2 Node.js nvm oh-my-zsh Sublime Text Visual Studio Code Note In the case of Docker Desktop , the link provided points to the latest download supported for macOS Catalina since newer versions require at least Big Sur. Homebrew Packages \u00b6 Here's some commands related to homebrew that install some necessary packages: brew tap adoptopenjdk/openjdk brew install neofetch adoptopenjdk htop Jenkins Agent \u00b6 Jenkins agents do not need any special software to run (other than Java). However, it is necessary to enable SSH connections on macOS by heading to System Preferences > Sharing and enabling Remote Login .","title":"1. macOS Configuration"},{"location":"setting-up/mac-mini/mac-os-configuration/#macos-configuration","text":"Note This Mac Mini in macOS will operate mainly as a Jenkins agent. Since the installation of macOS is graphically based there will not be any step by step indications of how to do it. Instead, this page specifies the software installed on this computer.","title":"macOS Configuration"},{"location":"setting-up/mac-mini/mac-os-configuration/#applications","text":"Here's a list of all the applications installed on the Mac Mini in macOS. Chrome Remote Desktop Docker Desktop Google Chrome Homebrew iTerm2 Node.js nvm oh-my-zsh Sublime Text Visual Studio Code Note In the case of Docker Desktop , the link provided points to the latest download supported for macOS Catalina since newer versions require at least Big Sur.","title":"Applications"},{"location":"setting-up/mac-mini/mac-os-configuration/#homebrew-packages","text":"Here's some commands related to homebrew that install some necessary packages: brew tap adoptopenjdk/openjdk brew install neofetch adoptopenjdk htop","title":"Homebrew Packages"},{"location":"setting-up/mac-mini/mac-os-configuration/#jenkins-agent","text":"Jenkins agents do not need any special software to run (other than Java). However, it is necessary to enable SSH connections on macOS by heading to System Preferences > Sharing and enabling Remote Login .","title":"Jenkins Agent"},{"location":"setting-up/mac-mini/windows-configuration/","text":"Windows Configuration \u00b6 Note This Mac Mini in Windows will operate mainly for game servers. Since the installation of Windows is graphically based there will not be any step by step indications of how to do it. Instead, this page specifies the software installed on this computer. Applications \u00b6 Here's a list of all the applications installed on the Mac Mini in macOS. Chrome Remote Desktop Google Chrome Java SE Python SharpKeys Steam SteamCMD Visual Studio Code VLC Windows Terminal (Download from Microsoft Store) Winrar ZeroTier Note SharpKeys is a program that allows you to remap keys in your keyboard. I use it mostly to switch the Z and Y keys on my keyboard while keeping my own keyboard layout. Dependencies \u00b6 Since some game servers require some redistributables, here's the download links for them: DirectX VCRedist 2015, 2017, 2019 and 2022 VCRedist All-in-One The last download contains all the Visual C++ Redistributables except for 2022. It also comes with a handy script that installs all of them automatically. Scoop \u00b6 Scoop is a package manager for Windows which can be installed by running the following command in an elevated powershell terminal: iwr -useb get . scoop . sh | iex Here's some commands related to scoop that install some necessary packages: scoop install git neofetch","title":"3. Windows Configuration"},{"location":"setting-up/mac-mini/windows-configuration/#windows-configuration","text":"Note This Mac Mini in Windows will operate mainly for game servers. Since the installation of Windows is graphically based there will not be any step by step indications of how to do it. Instead, this page specifies the software installed on this computer.","title":"Windows Configuration"},{"location":"setting-up/mac-mini/windows-configuration/#applications","text":"Here's a list of all the applications installed on the Mac Mini in macOS. Chrome Remote Desktop Google Chrome Java SE Python SharpKeys Steam SteamCMD Visual Studio Code VLC Windows Terminal (Download from Microsoft Store) Winrar ZeroTier Note SharpKeys is a program that allows you to remap keys in your keyboard. I use it mostly to switch the Z and Y keys on my keyboard while keeping my own keyboard layout.","title":"Applications"},{"location":"setting-up/mac-mini/windows-configuration/#dependencies","text":"Since some game servers require some redistributables, here's the download links for them: DirectX VCRedist 2015, 2017, 2019 and 2022 VCRedist All-in-One The last download contains all the Visual C++ Redistributables except for 2022. It also comes with a handy script that installs all of them automatically.","title":"Dependencies"},{"location":"setting-up/mac-mini/windows-configuration/#scoop","text":"Scoop is a package manager for Windows which can be installed by running the following command in an elevated powershell terminal: iwr -useb get . scoop . sh | iex Here's some commands related to scoop that install some necessary packages: scoop install git neofetch","title":"Scoop"}]}